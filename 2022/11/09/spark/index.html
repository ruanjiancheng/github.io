<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="Hexo Theme Keep">
    <meta name="description" content="write something interesting">
    <meta name="author" content="auggie">
    
    <title>
        
            spark |
        
        Auggie&#39;s blog
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    <link rel="shortcut icon" href="/images/logo.png">
    
<link rel="stylesheet" href="/fontawesome/css/fontawesome.min.css">

    
<link rel="stylesheet" href="/fontawesome/css/regular.min.css">

    
<link rel="stylesheet" href="/fontawesome/css/solid.min.css">

    
<link rel="stylesheet" href="/fontawesome/css/brands.min.css">

    <script id="hexo-configurations">
    let KEEP = window.KEEP || {};
    KEEP.hexo_config = {"hostname":"example.com","root":"/","language":"zh","path":"search.xml"};
    KEEP.theme_config = {"toc":{"enable":true,"number":false,"expand_all":false,"init_open":false},"style":{"primary_color":"#0066CC","logo":"/images/logo.svg","favicon":"/images/logo.png","avatar":"/images/avatar.png","font_size":null,"font_family":null,"left_side_width":"260px","content_max_width":"920px","hover":{"shadow":false,"scale":false},"first_screen":{"enable":true,"header_transparent":true,"background_img":"/images/bg.svg","description":"Keep running and Keep loving.","font_color":null,"hitokoto":{"enable":false}},"scroll":{"progress_bar":{"enable":true},"percent":{"enable":true}}},"local_search":{"enable":false,"preload":false},"code_copy":{},"code_block_tools":{"enable":true,"style":"default"},"side_tools":{},"pjax":{"enable":false},"lazyload":{"enable":false},"version":"3.4.9"};
    KEEP.language_ago = {"second":"%s seconds ago","minute":"%s minutes ago","hour":"%s hours ago","day":"%s days ago","week":"%s weeks ago","month":"%s months ago","year":"%s years ago"};
    KEEP.language_code_block = {"copy":"Copy code","copied":"Copied","fold":"Fold code block","folded":"Folded"};
  </script>
<meta name="generator" content="Hexo 5.4.2"></head>


<body>
<div class="progress-bar-container">
    
        <span class="scroll-progress-bar"></span>
    

    
</div>


<main class="page-container">

    

    <div class="page-main-content">

        <div class="page-main-content-top">
            
<header class="header-wrapper">

    <div class="header-content">
        <div class="left">
            
                <a class="logo-image" href="/">
                    <img src="/images/logo.svg">
                </a>
            
            <a class="logo-title" href="/">
               Auggie&#39;s blog
            </a>
        </div>

        <div class="right">
            <div class="pc">
                <ul class="menu-list">
                    
                        <li class="menu-item">
                            <a class=""
                               href="/"
                            >
                                HOME
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/archives"
                            >
                                ARCHIVES
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/about"
                            >
                                ABOUT
                            </a>
                        </li>
                    
                    
                </ul>
            </div>
            <div class="mobile">
                
                <div class="icon-item menu-bar">
                    <div class="menu-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <div class="header-drawer">
        <ul class="drawer-menu-list">
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/">HOME</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/archives">ARCHIVES</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/about">ABOUT</a>
                </li>
            
        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="page-main-content-middle">

            <div class="main-content">

                
                    <div class="fade-in-down-animation">
    <div class="article-content-container">

        <div class="article-title">
            <span class="title-hover-animation">spark</span>
        </div>

        
            <div class="article-header">
                <div class="avatar">
                    <img src="/images/avatar.png">
                </div>
                <div class="info">
                    <div class="author">
                        <span class="name">auggie</span>
                        
                    </div>
                    <div class="meta-info">
                        
<div class="article-meta-info">
    <span class="article-date article-meta-item">
        
            <i class="fa-regular fa-calendar-plus"></i>&nbsp;
        
        <span class="pc">2022-11-09 09:40:26</span>
        <span class="mobile">2022-11-09 09:40</span>
    </span>
    
        <span class="article-update-date article-meta-item">
        <i class="fas fa-file-pen"></i>&nbsp;
        <span class="pc">2022-12-04 19:26:53</span>
    </span>
    
    
    
        <span class="article-tags article-meta-item">
            <i class="fas fa-tags"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/tags/tech/">tech</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    

    
    
        <span class="article-wordcount article-meta-item">
            <i class="fas fa-file-word"></i>&nbsp;<span>10.6k Words</span>
        </span>
    
    
        <span class="article-min2read article-meta-item">
            <i class="fas fa-clock"></i>&nbsp;<span>53 Mins</span>
        </span>
    
    
</div>

                    </div>
                </div>
            </div>
        

        <div class="article-content markdown-body">
            <p><a class="link" target="_blank" rel="noopener" href="https://ihainan.gitbooks.io/spark-source-code/content/index.html">apache spark 源码阅读<i class="fas fa-external-link-alt"></i></a></p>
<p><a class="link" target="_blank" rel="noopener" href="https://dblab.xmu.edu.cn/blog/924/">spark 入门教程<i class="fas fa-external-link-alt"></i></a></p>
<p><a class="link" target="_blank" rel="noopener" href="https://spark.apache.org/">apache spark<i class="fas fa-external-link-alt"></i></a></p>
<h2 id="环境安装"><a href="#环境安装" class="headerlink" title="环境安装"></a>环境安装</h2><h3 id="hadoop"><a href="#hadoop" class="headerlink" title="hadoop"></a>hadoop</h3><blockquote>
<p>ssh免密码登陆</p>
</blockquote>
<ol>
<li>在mac的<strong>系统偏好设置–&gt;共享</strong>中打开远程登录</li>
<li><code>cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</code></li>
<li><code>ssh localhost</code></li>
</ol>
<blockquote>
<p>hadoop</p>
</blockquote>
<p><a class="link" target="_blank" rel="noopener" href="https://blog.csdn.net/pgs1004151212/article/details/104391391">hadoop mac 配置<i class="fas fa-external-link-alt"></i></a></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install hadoop</span><br></pre></td></tr></table></figure>

<p>Hadoop 伪分布式配置：</p>
<ol>
<li><code>/opt/homebrew/Cellar/hadoop/3.3.4/libexec/etc/hadoop/core-site.xml</code> 配置如下信息：</li>
</ol>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/opt/homebrew/Cellar/hadoop/3.3.4/libexec/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://localhost:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ol start="2">
<li><code>/opt/homebrew/Cellar/hadoop/3.3.4/libexec/etc/hadoop/hdfs-site.xml</code> 配置信息如下：</li>
</ol>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/opt/homebrew/Cellar/hadoop/3.3.4/libexec/tmp/dfs/nam    e<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/opt/homebrew/Cellar/hadoop/3.3.4/libexec/tmp/dfs/dat    a<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ol start="3">
<li>Env 配置</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">export CLASSPAHT=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar</span><br><span class="line">149 export HADOOP_HOME=/opt/homebrew/Cellar/hadoop/3.3.4/libexec</span><br><span class="line">150 export HADOOP_COMMON_HOME=$HADOOP_HOME</span><br><span class="line">151 export PATH=$JAVA_HOME/bin:$PATH:$HADOOP_HOME/bin:/opt//homebrew/Cellar/scala/bin</span><br></pre></td></tr></table></figure>



<blockquote>
<p>运行 hadoop 程序</p>
</blockquote>
<ol>
<li><p>初始化：hdfs namenode -format </p>
</li>
<li><p><code>start-dfs.sh</code></p>
<p><a class="link" target="_blank" rel="noopener" href="https://stackoverflow.com/questions/44009058/even-though-jre-8-is-installed-on-my-mac-no-java-runtime-present-requesting-t">找不到 JRE<i class="fas fa-external-link-alt"></i></a></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">(base) ➜  ~ start-dfs.sh</span><br><span class="line">Starting namenodes on [localhost]</span><br><span class="line">localhost: WARNING: /opt/homebrew/Cellar/hadoop/3.3.4/libexec/logs does not exist. Creating.</span><br><span class="line">Starting datanodes</span><br><span class="line">Starting secondary namenodes [Y3Y54Q72DR]</span><br><span class="line">Y3Y54Q72DR: ssh: Could not resolve hostname y3y54q72dr: nodename nor servname provided, or not known</span><br><span class="line">2022-11-09 16:17:23,985 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看是否启动 <code>jps</code></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">(base) ➜  ~ jps</span><br><span class="line">86881</span><br><span class="line">56226 Jps</span><br><span class="line">22195</span><br><span class="line">55683 NameNode</span><br><span class="line">39907 NailgunRunner</span><br><span class="line">55786 DataNode</span><br><span class="line">43003 Launcher</span><br><span class="line">23211 Application</span><br><span class="line">30365</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看 namenode， <a class="link" target="_blank" rel="noopener" href="http://localhost:9870/dfshealth.html#tab-overview">http://localhost:9870/dfshealth.html#tab-overview<i class="fas fa-external-link-alt"></i></a></p>
</li>
<li><p>修改 <code>yarn</code> 配置文件。<code>/opt/homebrew/Cellar/hadoop/3.3.4/libexec/etc/hadoop/mapred-site.xml</code> 添加内容：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name</span><br><span class="line">        <span class="tag">&lt;/<span class="name">name</span>&gt;</span>	</span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>修改 <code>/opt/homebrew/Cellar/hadoop/3.3.4/libexec/etc/hadoop/yarn-site.xml</code></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.env-whitelist<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>启动 <code>yarn</code>，<code>start-yarn.sh</code></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(base) ➜  hadoop git:(stable) start-yarn.sh</span><br><span class="line">Starting resourcemanager</span><br><span class="line">Starting nodemanagers</span><br></pre></td></tr></table></figure>

<p>浏览器中打开：<a class="link" target="_blank" rel="noopener" href="http://localhost:8088/cluster">http://localhost:8088/cluster<i class="fas fa-external-link-alt"></i></a></p>
</li>
<li><p>运行 Hadoop 自带的 wordcount 程序</p>
<ul>
<li><code>hadoop fs -mkdir /input</code> 创建文件夹</li>
<li><code>hadoop fs -ls /</code> 查看文件夹</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(base) ➜  hadoop git:(stable) hadoop fs -mkdir /input</span><br><span class="line">2022-11-09 16:27:09,642 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">(base) ➜  hadoop git:(stable) hadoop fs -ls /</span><br><span class="line">2022-11-09 16:27:30,749 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">Found 1 items</span><br><span class="line">drwxr-xr-x   - bytedance supergroup          0 2022-11-09 16:27 /input</span><br></pre></td></tr></table></figure>
</li>
<li><p>运行程序 <code>hadoop jar /opt/homebrew/Cellar/hadoop/3.3.4/libexec/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.4.jar   wordcount /input /output</code></p>
</li>
</ol>
<h3 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h3><p>太麻烦了（</p>
<p><a class="link" target="_blank" rel="noopener" href="https://www.yiibai.com/hive/hive_installation.html">Hive install<i class="fas fa-external-link-alt"></i></a></p>
<p><a class="link" target="_blank" rel="noopener" href="https://www.jianshu.com/p/3fef90437a9c">Hive install<i class="fas fa-external-link-alt"></i></a></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">export HIVE_HOME=/usr/local/hive</span><br><span class="line">export PATH=$PATH:$HIVE_HOME/bin</span><br><span class="line">export CLASSPATH=$CLASSPATH:/usr/local/Hadoop/lib/*:.</span><br><span class="line">export CLASSPATH=$CLASSPATH:/usr/local/hive/lib/*:.</span><br></pre></td></tr></table></figure>



<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">HADOOP_HOME=/opt/homebrew/Cellar/hadoop/3.3.4/libexec</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Hive Configuration Directory can be controlled by:</span></span><br><span class="line">export HIVE_CONF_DIR=/usr/local/hive/conf</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Folder containing extra libraries required <span class="keyword">for</span> hive compilation/    execution can be controlled by:</span></span><br><span class="line">export HIVE_AUX_JARS_PATH=/usr/local/hive/lib</span><br></pre></td></tr></table></figure>



<p><code>hive-site.xml</code></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.exec.scratchdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://master:8020/data/hive/temp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://master:8020/data/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.querylog.location<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://master:8020/data/hive/log<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    &lt;!—该配置是关闭hive元数据版本认证，否则会在启动spark程序时报错--&gt;</span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.schema.verification<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>





<h3 id="spark"><a href="#spark" class="headerlink" title="spark"></a>spark</h3><p><a class="link" target="_blank" rel="noopener" href="https://blog.csdn.net/vbirdbest/article/details/104499826">csdn spark<i class="fas fa-external-link-alt"></i></a></p>
<p><a class="link" target="_blank" rel="noopener" href="https://dblab.xmu.edu.cn/blog/1307/">xmu spark<i class="fas fa-external-link-alt"></i></a></p>
<ol>
<li><p>安装 <code>scala</code>，必须安装 spark 指定的版本，不然 spark 会报错</p>
</li>
<li><p>下载 <code>spark</code>，<a class="link" target="_blank" rel="noopener" href="https://spark.apache.org/downloads.html">spark官网<i class="fas fa-external-link-alt"></i></a></p>
</li>
<li><p>安装 配置 <code>spark</code></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo tar -zxf ~/下载/spark-2.1.0-bin-without-hadoop.tgz -C /usr/local/cd /usr/local</span><br><span class="line">sudo <span class="built_in">mv</span> ./spark-2.1.0-bin-without-hadoop/ ./spark</span><br><span class="line">sudo <span class="built_in">chown</span> -R hadoop:hadoop ./spark          <span class="comment"># 此处的 hadoop 为你的用户名</span></span><br></pre></td></tr></table></figure>

<p>安装后，还需要修改Spark的配置文件spark-env.sh</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> <span class="variable">$SPARK_HOME</span></span><br><span class="line"><span class="built_in">cp</span> ./conf/spark-env.sh.template ./conf/spark-env.sh</span><br></pre></td></tr></table></figure>

<p>编辑spark-env.sh文件(vim .&#x2F;conf&#x2F;spark-env.sh)，在第一行添加以下配置信息:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export SPARK_DIST_CLASSPATH=$(/opt/homebrew/bin/hadoop classpath)</span><br></pre></td></tr></table></figure>

<p>有了上面的配置信息以后，Spark就可以把数据存储到Hadoop分布式文件系统HDFS中，也可以从HDFS中读取数据。如果没有配置上面信息，Spark就只能读写本地数据，无法读写HDFS数据。</p>
</li>
<li><p>验证安装</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">bin/run-example SparkPi 2&gt;&amp;1 | grep &quot;Pi is&quot;</span><br><span class="line"></span><br><span class="line">(base) ➜  spark-3.3.1-bin-hadoop3 git:(stable) bin/run-example SparkPi 2&gt;&amp;1 | grep &quot;Pi is&quot;</span><br><span class="line">Pi is roughly 3.1459757298786495</span><br></pre></td></tr></table></figure>
</li>
<li><p>启动 spark</p>
<p><a class="link" target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_43042683/article/details/105397046">localhost: namenode running as process 2896. Stop it first.<i class="fas fa-external-link-alt"></i></a></p>
<p>不要启动到 <code>hadoop</code> 的 <code>start_all.sh</code> 了😇</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">(base) ➜  ~ cd $SPARK_HOME/sbin</span><br><span class="line">(base) ➜  sbin git:(stable) ./start-all.sh</span><br><span class="line">starting org.apache.spark.deploy.master.Master, logging to /opt/homebrew/Cellar/spark-3.3.1-bin-hadoop3/logs/spark-bytedance-org.apache.spark.deploy.master.Master-1-Y3Y54Q72DR.out</span><br><span class="line">localhost: starting org.apache.spark.deploy.worker.Worker, logging to /opt/homebrew/Cellar/spark-3.3.1-bin-hadoop3/logs/spark-bytedance-org.apache.spark.deploy.worker.Worker-1-Y3Y54Q72DR.out</span><br><span class="line">(base) ➜  sbin git:(stable) jps</span><br><span class="line">86881</span><br><span class="line">67523 Master</span><br><span class="line">22195</span><br><span class="line">67634 Worker</span><br><span class="line">67669 Jps</span><br><span class="line">66823 NameNode</span><br><span class="line">43003 Launcher</span><br><span class="line">23211 Application</span><br><span class="line">67197 ResourceManager</span><br><span class="line">30365</span><br><span class="line">66927 DataNode</span><br></pre></td></tr></table></figure>

<p><a class="link" target="_blank" rel="noopener" href="http://localhost:8080/">spark 本地地址<i class="fas fa-external-link-alt"></i></a></p>
</li>
<li><p>启动 <code>spark shell</code></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">spark-shell --master &lt;master-url&gt;</span><br><span class="line"></span><br><span class="line">* local 使用一个Worker线程本地化运行SPARK(完全不并行)</span><br><span class="line">* local[*] 使用逻辑CPU个数数量的线程来本地化运行Spark</span><br><span class="line">* local[K] 使用K个Worker线程本地化运行Spark（理想情况下，K应该根据运行机器的CPU核数设定）</span><br><span class="line">* spark://HOST:PORT 连接到指定的Spark standalone master。默认端口是7077.</span><br><span class="line">* yarn-client 以客户端模式连接YARN集群。集群的位置可以在HADOOP_CONF_DIR 环境变量中找到。</span><br><span class="line">* yarn-cluster 以集群模式连接YARN集群。集群的位置可以在HADOOP_CONF_DIR 环境变量中找到。</span><br><span class="line">* mesos://HOST:PORT 连接到指定的Mesos集群。默认接口是5050。</span><br></pre></td></tr></table></figure>



<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd $SPARK_HOME/bin</span><br><span class="line">./spark-shell --master local</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Welcome to</span><br><span class="line">      ____              __</span><br><span class="line">     / __/__  ___ _____/ /__</span><br><span class="line">    _\ \/ _ \/ _ `/ __/  &#x27;_/</span><br><span class="line">   /___/ .__/\_,_/_/ /_/\_\   version 3.3.1</span><br><span class="line">      /_/</span><br><span class="line"></span><br><span class="line">Using Scala version 2.12.15 (OpenJDK 64-Bit Server VM, Java 1.8.0_292)</span><br><span class="line">Type in expressions to have them evaluated.</span><br><span class="line">Type :help for more information.</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">scala&gt;</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="Spark-独立应用程序编程"><a href="#Spark-独立应用程序编程" class="headerlink" title="Spark 独立应用程序编程"></a>Spark 独立应用程序编程</h3><h4 id="遇到的坑"><a href="#遇到的坑" class="headerlink" title="遇到的坑"></a>遇到的坑</h4><ul>
<li><p>路径问题：</p>
<p>spark 应该会从 hdfs 上面查找文件。如果需要查找本地文件的话，需要使用 <code>file:</code> 开头，例如：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> logFile = <span class="string">&quot;file:/Users/bytedance/resource/fyy/log.log&quot;</span></span><br></pre></td></tr></table></figure>


</li>
<li><p>找不到 class 的问题：</p>
<ul>
<li><p>首先包含 <code>main</code> 方法的类名要和 <code>--class</code> 输入的类名保持一致。</p>
</li>
<li><p>使用 idea 会报错，不知道为什么。之后还是用 vscode 吧😇</p>
<p>如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]</span><br><span class="line">Error: Failed to load class SimpleApp.</span><br><span class="line">22/11/10 11:22:42 INFO ShutdownHookManager: Shutdown hook called</span><br><span class="line">22/11/10 11:22:42 INFO ShutdownHookManager: Deleting directory /private/var/folders/b1/0fd1b6hs7lz0fm_mh346lybm0000gn/T/spark-533682ae-17f9-4229-a3d0-a621fe441511</span><br></pre></td></tr></table></figure></li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SimpleApp</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="keyword">val</span> logFile = <span class="string">&quot;file:/Users/bytedance/resource/fyy/log.log&quot;</span> <span class="comment">// Should be some file on your system</span></span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder.appName(<span class="string">&quot;Simple Application&quot;</span>).getOrCreate()</span><br><span class="line">    <span class="keyword">val</span> logData = spark.read.textFile(logFile).cache()</span><br><span class="line">    <span class="keyword">val</span> numAs = logData.filter(line =&gt; line.contains(<span class="string">&quot;a&quot;</span>)).count()</span><br><span class="line">    <span class="keyword">val</span> numBs = logData.filter(line =&gt; line.contains(<span class="string">&quot;b&quot;</span>)).count()</span><br><span class="line">    println(<span class="string">s&quot;Lines with a: <span class="subst">$numAs</span>, Lines with b: <span class="subst">$numBs</span>&quot;</span>)</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">name := <span class="string">&quot;Simple App&quot;</span></span><br><span class="line"></span><br><span class="line">version := <span class="string">&quot;1.0&quot;</span></span><br><span class="line"></span><br><span class="line">scalaVersion := <span class="string">&quot;2.13.10&quot;</span></span><br><span class="line"></span><br><span class="line">libraryDependencies += <span class="string">&quot;org.apache.spark&quot;</span> %% <span class="string">&quot;spark-sql&quot;</span> % <span class="string">&quot;3.3.1&quot;</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="两种-sparkcontext-姿势"><a href="#两种-sparkcontext-姿势" class="headerlink" title="两种 sparkcontext 姿势"></a>两种 sparkcontext 姿势</h4><p><a class="link" target="_blank" rel="noopener" href="https://blog.csdn.net/qq_35235032/article/details/109222246">sparkcontext 配置 hdfs<i class="fas fa-external-link-alt"></i></a></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span>._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SimpleApp</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local&quot;</span>).setAppName(<span class="string">&quot;SimpleApp&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="keyword">val</span> filename = <span class="string">&quot;hdfs://localhost:8020/user/spark/word.txt&quot;</span></span><br><span class="line">    <span class="keyword">val</span> txt = sc.textFile(filename)</span><br><span class="line">    <span class="keyword">val</span> res = txt.flatMap(line =&gt; line.split(<span class="string">&quot; &quot;</span>)).filter(_.size &gt; <span class="number">0</span>).map(word =&gt; (word -&gt; <span class="number">1</span>))</span><br><span class="line">    .reduceByKey(_ + _).collect()</span><br><span class="line">    res.foreach(println)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SimpleApp</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().config(<span class="string">&quot;spark.master&quot;</span>, <span class="string">&quot;local&quot;</span>).getOrCreate()</span><br><span class="line">    <span class="keyword">val</span> sc = spark.sparkContext</span><br><span class="line">    <span class="keyword">val</span> filename = <span class="string">&quot;hdfs://localhost:8020/user/spark/word.txt&quot;</span></span><br><span class="line">    <span class="keyword">val</span> txt = sc.textFile(filename)</span><br><span class="line">    <span class="keyword">val</span> res = txt.flatMap(line =&gt; line.split(<span class="string">&quot; &quot;</span>)).filter(_.size &gt; <span class="number">0</span>).map(word =&gt; (word -&gt; <span class="number">1</span>))</span><br><span class="line">    .reduceByKey(_ + _).collect()</span><br><span class="line">    res.foreach(println)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>





<p>输出：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">(base) ➜  sparkapp $SPARK_HOME/bin/spark-submit \</span><br><span class="line">--class &quot;SimpleApp&quot; \</span><br><span class="line">--master local \</span><br><span class="line">./target/scala-2.13/simple-app_2.13-1.0.jar 2&amp;&gt;1 | grep &quot;Line&quot;</span><br><span class="line">Lines with a: 16, Lines with b: 15</span><br></pre></td></tr></table></figure>

<h4 id="wordCount"><a href="#wordCount" class="headerlink" title="wordCount"></a>wordCount</h4><blockquote>
<p>一些常用的函数</p>
</blockquote>
<p>文件读写：</p>
<ul>
<li>读写本地文件</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 读取文件	如果是本地文件 必须用 file:// 开头，用于区分 hdfs</span></span><br><span class="line"><span class="keyword">val</span> file = sc.textFile(filepath)</span><br><span class="line"></span><br><span class="line"><span class="comment">// spark 懒加载，加载文件的第一行</span></span><br><span class="line">file.first()</span><br><span class="line"></span><br><span class="line"><span class="comment">// textFile 写会</span></span><br><span class="line">file.saveasTextFile(filepath)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<ul>
<li>读写 hdfs 文件</li>
</ul>
<p>hdfs 常用命令：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -mkdir hdfspath</span><br><span class="line"></span><br><span class="line">hdfs dfs -ls hdfspath</span><br><span class="line"></span><br><span class="line">hdfs dfs -put localpath hfdspath</span><br><span class="line"></span><br><span class="line">hdfs dfs -cat hdfsfile</span><br><span class="line"></span><br><span class="line">hdfs dfs -get</span><br><span class="line"></span><br><span class="line">hdfs getconf -confKey fs.default.name # 获取本地 hdfs 端口号</span><br></pre></td></tr></table></figure>

<p>spark 读取 hdfs 路径：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> textFile = sc.textFile(<span class="string">&quot;hdfs://localhost:8020/user/spark/word.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 实际上可以省略不写</span></span><br><span class="line"><span class="keyword">val</span> textFile = sc.textFile(<span class="string">&quot;/user/spark/word.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 读写操作和本地文件类似</span></span><br></pre></td></tr></table></figure>



<p>词频统计：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">textFile.flatMap(line =&gt; line.split(<span class="string">&quot; &quot;</span>)).map(word =&gt; (word, <span class="number">1</span>)).reduceByKey((a, b) =&gt; a + b).foreach(println)</span><br></pre></td></tr></table></figure>



<h3 id="配置-vscode-scala-spark-环境"><a href="#配置-vscode-scala-spark-环境" class="headerlink" title="配置 vscode scala spark 环境"></a>配置 vscode scala spark 环境</h3><p><a class="link" target="_blank" rel="noopener" href="https://blog.csdn.net/lyd882/article/details/111638953">vscode scala<i class="fas fa-external-link-alt"></i></a></p>
<h4 id="vscode-配置-scalafmt-寄了，建议用-idea"><a href="#vscode-配置-scalafmt-寄了，建议用-idea" class="headerlink" title="vscode 配置 scalafmt (寄了，建议用 idea)"></a>vscode 配置 scalafmt (寄了，建议用 idea)</h4><p><a class="link" target="_blank" rel="noopener" href="https://scalameta.org/scalafmt/docs/installation.html#metals">vscode scalafmt<i class="fas fa-external-link-alt"></i></a></p>
<p>在 <code>project</code> 中添加 <code>plugins.sbt</code> 文件</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// In project/plugins.sbt. Note, does not support sbt 0.13, only sbt 1.x.</span></span><br><span class="line">addSbtPlugin(<span class="string">&quot;org.scalameta&quot;</span> % <span class="string">&quot;sbt-scalafmt&quot;</span> % <span class="type">SBT_PLUGIN_VERSION</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// idea</span></span><br><span class="line">addSbtPlugin(<span class="string">&quot;org.jetbrains.scala&quot;</span> % <span class="string">&quot;sbt-ide-settings&quot;</span> % <span class="string">&quot;1.1.1&quot;</span>)</span><br></pre></td></tr></table></figure>



<p>添加 <code>.scalafmt.config</code> 文件</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">version = <span class="number">2.7</span><span class="number">.5</span></span><br></pre></td></tr></table></figure>



<h2 id="Spark-运行流程"><a href="#Spark-运行流程" class="headerlink" title="Spark 运行流程"></a>Spark 运行流程</h2><p><a class="link" target="_blank" rel="noopener" href="https://www.cnblogs.com/frankdeng/p/9301485.html">spark 运行流程<i class="fas fa-external-link-alt"></i></a></p>
<p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180425170820868-121220770.png" alt="spark-process"></p>
<ol>
<li>Spark 应用程序被提交，任务控制节点(Driver)创建一个 SparkContext。SparkContext会向资源管理器(Cluster Manager)注册并申请运行Executor的资源。</li>
<li>Cluster Manager 为 Executor 分配资源，启动 executor 进程，executor运行情况将随着“心跳”发送到资源管理器上。</li>
<li>RDD -&gt; DAG -&gt; task set(stage) -&gt; executor</li>
<li>任务在Executor上运行，把执行结果反馈给任务调度器，然后反馈给DAG调度器，运行完毕后写入数据并释放所有资源。</li>
</ol>
<p>一个任务创建在一个 worker 上面创建一个 exector</p>
<h2 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h2><p>一个RDD就是一个<strong>分布式对象集合</strong>，本质上是一个<strong>只读</strong>的分区记录集合，每个RDD可以分成多个分区，每个分区就是一个数据集片段，并且一个RDD的<strong>不同分区可以被保存到集群中不同的节点上，从而可以在集群中的不同节点上进行并行计算。</strong></p>
<ul>
<li><p>容错：通过 RDD 的 DAG 实现容错</p>
</li>
<li><p>中间结果持久化到内存</p>
</li>
<li><p>存放数据可以是 Java obj</p>
</li>
<li><p>Narrow：一对一，多对一</p>
</li>
<li><p>Wild：一对多</p>
</li>
</ul>
<p><img src="https://dblab.xmu.edu.cn/blog/wp-content/uploads/2016/11/%E5%9B%BE9-10-%E7%AA%84%E4%BE%9D%E8%B5%96%E4%B8%8E%E5%AE%BD%E4%BE%9D%E8%B5%96%E7%9A%84%E5%8C%BA%E5%88%AB.jpg" alt="narrow &amp; wild"></p>
<blockquote>
<p>阶段划分</p>
</blockquote>
<p>在DAG中进行反向解析，<strong>遇到宽依赖就断开</strong>，<strong>遇到窄依赖就把当前的RDD加入到当前的阶段中</strong>；将窄依赖尽量划分在同一个阶段中，可以实现流水线计算。</p>
<p><img src="https://dblab.xmu.edu.cn/blog/wp-content/uploads/2016/11/%E5%9B%BE9-11-%E6%A0%B9%E6%8D%AERDD%E5%88%86%E5%8C%BA%E7%9A%84%E4%BE%9D%E8%B5%96%E5%85%B3%E7%B3%BB%E5%88%92%E5%88%86%E9%98%B6%E6%AE%B5.jpg" alt="划分"></p>
<blockquote>
<p>运行过程</p>
</blockquote>
<ul>
<li>创建 RDD，sparkContext 解析依赖关系，生成 DAG</li>
<li>DAGScheduler 将 DAG 拆成多个 stage。</li>
<li>每个阶段中包含了多个任务，每个任务会被任务调度器分发给各个工作节点（Worker Node）上的Executor去执行。</li>
</ul>
<p><img src="https://dblab.xmu.edu.cn/blog/wp-content/uploads/2016/11/%E5%9B%BE9-12-RDD%E5%9C%A8Spark%E4%B8%AD%E7%9A%84%E8%BF%90%E8%A1%8C%E8%BF%87%E7%A8%8B.jpg" alt="process"></p>
<h3 id="RDD创建"><a href="#RDD创建" class="headerlink" title="RDD创建"></a>RDD创建</h3><ul>
<li><p>读取外部数据集，HDFS，LOCAL，Kafka</p>
<p>如果使用了本地文件系统的路径，那么，<strong>必须要保证在所有的worker节点上，也都能够采用相同的路径访问到该文件，</strong>比如，可以把该文件拷贝到每个worker节点上，或者也可以使用网络挂载共享文件系统。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> lines = sc.textFile(<span class="string">&quot;file:///usr/local/spark/mycode/rdd/word.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> lines = sc.textFile(<span class="string">&quot;hdfs://localhost:9000/user/hadoop/word.txt&quot;</span>)</span><br></pre></td></tr></table></figure>


</li>
<li><p>调用 SparkContext 的 parallelize 方法，在已有的集合上面创建一个 RDD</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list = (<span class="number">1</span> to <span class="number">10</span>).toList</span><br><span class="line"><span class="keyword">val</span> array = (<span class="number">1</span> to <span class="number">10</span>).toArray</span><br><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(list)</span><br><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(array)</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="RDD操作"><a href="#RDD操作" class="headerlink" title="RDD操作"></a>RDD操作</h3><p>RDD 创建之后，在后续过程中一般只会发生两种操作：</p>
<ul>
<li>transformation：RDD &#x3D;&gt; newRDD</li>
<li>Action: 在数据集上运算，返回结果</li>
</ul>
<h4 id="transformation"><a href="#transformation" class="headerlink" title="transformation"></a>transformation</h4><ul>
<li>filter(func)</li>
<li>map(func)</li>
<li>flatMap(func)</li>
<li>groupByKey()</li>
<li>reduceByKey(func)</li>
</ul>
<h4 id="action"><a href="#action" class="headerlink" title="action"></a>action</h4><ul>
<li>count() </li>
<li>collect() 以数组的形式返回数据集中的所有元素</li>
<li>first() </li>
<li>take(n)</li>
<li>reduce(func)</li>
<li>foreach(func)</li>
</ul>
<h3 id="RDD持久化"><a href="#RDD持久化" class="headerlink" title="RDD持久化"></a>RDD持久化</h3><p>RDD采用惰性求值的机制，每次遇到行动操作，<strong>都会从头开始执行计算</strong>。</p>
<p>在一些情形下，我们需要多次调用不同的行动操作，这就意味着，每次调用行动操作，都会触发一次从头开始的计算。</p>
<p>通过持久化机制避免这种重复计算的开销。可以使用persist()方法对一个RDD标记为持久化，之所以说“标记为持久化”，是因为出现persist()语句的地方，<strong>并不会马上计算生成RDD并把它持久化，而是要等到遇到第一个行动操作触发真正计算以后</strong>，才会把计算结果进行持久化，持久化后的RDD将会被保留在计算节点的内存中被后面的行动操作重复使用。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> mp = textFile.map(line =&gt; line.split(<span class="string">&quot; &quot;</span>).size)</span><br><span class="line">mp: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">48</span>] at map at &lt;console&gt;:<span class="number">23</span></span><br><span class="line"></span><br><span class="line">scala&gt; mp.cache() <span class="comment">// 不会缓存 mp，因为 mp 还没有计算生成</span></span><br><span class="line">res32: mp.<span class="keyword">type</span> = <span class="type">MapPartitionsRDD</span>[<span class="number">48</span>] at map at &lt;console&gt;:<span class="number">23</span></span><br><span class="line"></span><br><span class="line">scala&gt; mp.count() <span class="comment">// 计算 mp，并且缓存</span></span><br><span class="line">res33: <span class="type">Long</span> = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">scala&gt; mp.collect() <span class="comment">// 重复使用 cache 的 mp</span></span><br><span class="line">res34: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">7</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure>



<p>如果使用集群模式打印 RDD 的话，不能使用  <code>rdd.foreach(println)</code> 或者<code>rdd.map(println)</code>，而是 <code>rdd.collect().foreach(println)</code> 或者<code>rdd.collect().toke(100).foreach(println)</code></p>
<h3 id="键值对-RDD"><a href="#键值对-RDD" class="headerlink" title="键值对 RDD"></a>键值对 RDD</h3><p>通过 map 函数创建 kvRDD</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> pairRDD = lines.flatMap(line =&gt; line.split(<span class="string">&quot; &quot;</span>)).map(word =&gt; (word,<span class="number">1</span>))</span><br></pre></td></tr></table></figure>



<p>transformation:</p>
<ul>
<li><p>reduceByKey</p>
</li>
<li><p>groupByKey</p>
</li>
<li><p>sortByKey</p>
</li>
<li><p>mapValues(func): 作用于 (k, v) 的 value 的函数</p>
</li>
<li><p>join:  (k, v1), (k, v2) &#x3D;&gt; (k, (v1, v2))</p>
</li>
<li><p>keys</p>
</li>
<li><p>values</p>
</li>
</ul>
<p>demo：统计 kv 的平均值</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(<span class="type">Array</span>((<span class="string">&quot;spark&quot;</span>,<span class="number">2</span>),(<span class="string">&quot;hadoop&quot;</span>,<span class="number">6</span>),(<span class="string">&quot;hadoop&quot;</span>,<span class="number">4</span>),(<span class="string">&quot;spark&quot;</span>,<span class="number">6</span>)))</span><br><span class="line"></span><br><span class="line">rdd.mapValues(x =&gt; (x, <span class="number">1</span>)).reduceByKey((x, y) =&gt; (x._1 + y._1, x._2 + y._2)).mapValues(x =&gt; x._1 / x._2).collect()</span><br></pre></td></tr></table></figure>





<ul>
<li><p>API</p>
<ul>
<li><p>Transformations &#x3D;&gt; another RDD</p>
<ul>
<li>map, filter, union</li>
<li>groupByKey, reduceByKey, repartition</li>
</ul>
</li>
<li><p>Actions &#x3D;&gt; Lineage 中断</p>
<ul>
<li>count, collect, saveAsTextFile</li>
<li>foreach</li>
</ul>
</li>
<li><p>parallelize</p>
</li>
</ul>
</li>
<li><p>Source</p>
<ul>
<li><p>from storage</p>
<ul>
<li><p>HDFS <code>words = sc.textFile(&quot;hdfs://...&quot;)</code></p>
</li>
<li><p>local:</p>
</li>
<li><p>Kafka</p>
</li>
</ul>
</li>
<li><p>from another RDD</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">res = words.map(word =&gt; (word, <span class="number">1</span>)).reduceByKey((a, b) =&gt; a + b)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<p>提交作业：</p>
<ul>
<li><p><code>spark-shell</code></p>
</li>
<li><p><code>Spark-submit</code></p>
</li>
<li><p>narrow(pipeline): e.g. map</p>
</li>
<li><p>wide(shuffle): e.g. reduceByKey</p>
</li>
</ul>
<h2 id="共享变量"><a href="#共享变量" class="headerlink" title="共享变量"></a>共享变量</h2><p>当Spark在集群的多个不同节点的多个任务上并行运行一个函数时，它会把函数中涉及到的<strong>每个变量</strong>，在每个任务上都生成一个<strong>副本</strong>。</p>
<p>问题：有时候，需要在多个任务之间共享变量。</p>
<p>解决：引入 广播变量，累加器</p>
<h3 id="广播变量"><a href="#广播变量" class="headerlink" title="广播变量"></a>广播变量</h3><blockquote>
<p><strong>序列化</strong>是 Java对象 &#x3D;&gt; 字节序列 的过程。</p>
<p><strong>反序列化</strong>是 字节序列 &#x3D;&gt; Java对象 的过程。</p>
<p>主要用于两个 java 进程进行通信，传输 java 对象传送。</p>
<p>好处：</p>
<ul>
<li>数据持久化</li>
<li>序列化实现远程通信</li>
</ul>
<p><a class="link" target="_blank" rel="noopener" href="https://juejin.cn/post/7064942360106369054">序列化 &amp; 反序列化<i class="fas fa-external-link-alt"></i></a></p>
</blockquote>
<p>在每个机器上<strong>缓存</strong>一个<strong>只读</strong>的变量，<strong>而不是</strong>为机器上的每个任务都生成一个副本。</p>
<p>显式创建广播变量的场景：当跨越多个阶段的那些任务需要相同的数据。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> broadcastVar = sc.broadcast(<span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">broadcastVar.value</span><br></pre></td></tr></table></figure>

<p>SparkContext.broadcast(v) 之后，集群的函数都需要使用 广播变量 的值，而不是原值。</p>
<h3 id="累加器"><a href="#累加器" class="headerlink" title="累加器"></a>累加器</h3><p>一个数值型的累加器，可以通过调用SparkContext.longAccumulator()或者SparkContext.doubleAccumulator()来创建。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> accum = sc.longAccumulator(<span class="string">&quot;My Accumulator&quot;</span>)</span><br><span class="line"></span><br><span class="line">sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)).foreach(x =&gt; accum.add(x))</span><br><span class="line"></span><br><span class="line">accum.value</span><br></pre></td></tr></table></figure>



<h2 id="Spark-SQL"><a href="#Spark-SQL" class="headerlink" title="Spark SQL"></a>Spark SQL</h2><p>DataFrame 和 RDD 的区别：</p>
<ul>
<li>RDD 是分布式 Java 对象集合，对象内部结构对于 RDD 不可知。</li>
<li>DataFrame 是以 RDD 为基础的分布式数据集(分布式 Row 对象集合)，提供详细的结构信息</li>
</ul>
<p>同时：</p>
<p>DataFrame 也采用惰性机制，和 RDD 的处理逻辑一样。</p>
<h3 id="DF-创建-amp-保存"><a href="#DF-创建-amp-保存" class="headerlink" title="DF 创建 &amp; 保存"></a>DF 创建 &amp; 保存</h3><blockquote>
<p>创建</p>
</blockquote>
<ul>
<li>读取文件</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> peopleDF = spark.read.format(<span class="string">&quot;json&quot;</span>).load(<span class="string">&quot;file:///people.json&quot;</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li>使用 rdd 创建 df</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> filename = <span class="string">&quot;hdfs://localhost:8020/user/spark/people.txt&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> peopleRDD = spark.sparkContext.textFile(filename)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// The schema is encoded in a string</span></span><br><span class="line">    <span class="keyword">val</span> schemaString = <span class="string">&quot;name age&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Generate the schema based on the string of schema</span></span><br><span class="line">    <span class="keyword">val</span> fields = schemaString.split(<span class="string">&quot; &quot;</span>)</span><br><span class="line">      .map(fieldName =&gt; <span class="type">StructField</span>(fieldName, <span class="type">StringType</span>, nullable = <span class="literal">true</span>))</span><br><span class="line">    <span class="keyword">val</span> schema = <span class="type">StructType</span>(fields)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Convert records of the RDD (people) to Rows</span></span><br><span class="line">    <span class="keyword">val</span> rowRDD = peopleRDD</span><br><span class="line">      .map(_.split(<span class="string">&quot;,&quot;</span>))</span><br><span class="line">      .map(attributes =&gt; <span class="type">Row</span>(attributes(<span class="number">0</span>), attributes(<span class="number">1</span>).trim))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Apply the schema to the RDD</span></span><br><span class="line">    <span class="keyword">val</span> peopleDF = spark.createDataFrame(rowRDD, schema)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Creates a temporary view using the DataFrame</span></span><br><span class="line">    peopleDF.createOrReplaceTempView(<span class="string">&quot;people&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// SQL can be run over a temporary view created using DataFrames</span></span><br><span class="line">    <span class="keyword">val</span> results = spark.sql(<span class="string">&quot;SELECT name FROM people&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// The results of SQL queries are DataFrames and support all the normal RDD operations</span></span><br><span class="line">    <span class="comment">// The columns of a row in the result can be accessed by field index or by field name</span></span><br><span class="line">    results.map(attributes =&gt; <span class="string">&quot;Name: &quot;</span> + attributes(<span class="number">0</span>)).show()</span><br></pre></td></tr></table></figure>

<ol>
<li>读取 rdd &#x3D;&gt; rowRDD</li>
<li>创建 schema</li>
<li>创建 df</li>
</ol>
<blockquote>
<p>保存</p>
</blockquote>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">peopleDF.select(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;age&quot;</span>).write.format(<span class="string">&quot;csv&quot;</span>).save(<span class="string">&quot;file:///newpeople.csv&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>write.format()支持输出 json,parquet, jdbc, orc, libsvm, csv, text等格式文件，如果要输出文本文件，可以采用write.format(“text”)，但是，需要注意，只有select()中只存在一个列时，才允许保存成文本文件，如果存在两个列，比如select(“name”, “age”)，就不能保存成文本文件。</p>
<h3 id="获取-mysql-数据"><a href="#获取-mysql-数据" class="headerlink" title="获取 mysql 数据"></a>获取 mysql 数据</h3><p><a class="link" target="_blank" rel="noopener" href="https://blog.csdn.net/hjw199089/article/details/53522554">shell &amp; 程序方法使用 JDBC 读取 mysql 数据<i class="fas fa-external-link-alt"></i></a></p>
<ul>
<li><p>安装 JDBC</p>
</li>
<li><p>启动 shell 的时候指定 jdbc</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spark-shell \</span><br><span class="line">--jars $SPARK_HOME/jars/mysql-connector-j-8.0.31/mysql-connector-j-8.0.31.jar \</span><br><span class="line">--driver-class-path $SPARK_HOME/jars/mysql-connector-j-8.0.31/mysql-connector-j-8.0.31.jar</span><br></pre></td></tr></table></figure>
</li>
<li><p>获取 mysql 数据</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> jdbcDF = spark.read.format(<span class="string">&quot;jdbc&quot;</span>).option(<span class="string">&quot;url&quot;</span>, <span class="string">&quot;jdbc:mysql://localhost:3306/spark&quot;</span>).option(<span class="string">&quot;driver&quot;</span>,<span class="string">&quot;com.mysql.jdbc.Driver&quot;</span>).option(<span class="string">&quot;dbtable&quot;</span>, <span class="string">&quot;student&quot;</span>).option(<span class="string">&quot;user&quot;</span>, <span class="string">&quot;root&quot;</span>).option(<span class="string">&quot;password&quot;</span>, <span class="string">&quot;123&quot;</span>).load()</span><br><span class="line">jdbcDF: org.apache.spark.sql.<span class="type">DataFrame</span> = [id: int, name: string ... <span class="number">2</span> more fields]</span><br><span class="line"></span><br><span class="line">scala&gt; jdbcDF.show()</span><br><span class="line">+---+--------+------+---+</span><br><span class="line">| id|    name|gender|age|</span><br><span class="line">+---+--------+------+---+</span><br><span class="line">|  <span class="number">1</span>| <span class="type">Xueqian</span>|     <span class="type">F</span>| <span class="number">23</span>|</span><br><span class="line">|  <span class="number">2</span>|<span class="type">Weiliang</span>|     <span class="type">M</span>| <span class="number">24</span>|</span><br><span class="line">+---+--------+------+---+</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="Spark-Streaming"><a href="#Spark-Streaming" class="headerlink" title="Spark Streaming"></a>Spark Streaming</h2><p><img src="https://spark.apache.org/docs/latest/img/streaming-flow.png"></p>
<p>将输入流按照时间片进行拆分（秒级），然后进过 spark 引擎批处理。</p>
<p>DStream（Discretized Stream，离散化数据流），表示连续不断的数据流。</p>
<h2 id="数据分区器-rdd如何存储"><a href="#数据分区器-rdd如何存储" class="headerlink" title="数据分区器 rdd如何存储"></a>数据分区器 rdd如何存储</h2><h2 id="Spark-源码阅读"><a href="#Spark-源码阅读" class="headerlink" title="Spark 源码阅读"></a>Spark 源码阅读</h2><h3 id="RDD-1"><a href="#RDD-1" class="headerlink" title="RDD"></a>RDD</h3><ul>
<li>partitions 列表</li>
<li>dependencies 列表</li>
<li>一些函数，e.g. map</li>
</ul>
<h4 id="RDD-分区"><a href="#RDD-分区" class="headerlink" title="RDD 分区"></a>RDD 分区</h4><p>分区接口定义，实现 <code>Serializable</code> 可以实现序列化操作。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Partition</span> <span class="keyword">extends</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Get the partition&#x27;s index within its parent RDD</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">index</span></span>: <span class="type">Int</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// A better default implementation of HashCode</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">hashCode</span></span>(): <span class="type">Int</span> = index</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">equals</span></span>(other: <span class="type">Any</span>): <span class="type">Boolean</span> = <span class="keyword">super</span>.equals(other)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p>RDD：</p>
<ul>
<li>包含一个 <code>partitions</code> 的 list，外界只能通过 <code>partitions</code> 方法来访问，子类需要重写 <code>getPartitions</code> 方法</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@transient</span> <span class="keyword">private</span> <span class="keyword">var</span> partitions_ : <span class="type">Array</span>[<span class="type">Partition</span>] = <span class="literal">null</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Implemented by subclasses to return the set of partitions in this RDD. This method will only</span></span><br><span class="line"><span class="comment">   * be called once, so it is safe to implement a time-consuming computation in it.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartitions</span></span>: <span class="type">Array</span>[<span class="type">Partition</span>]</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Get the array of partitions of this RDD, taking into account whether the</span></span><br><span class="line"><span class="comment">   * RDD is checkpointed or not.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">final</span> <span class="function"><span class="keyword">def</span> <span class="title">partitions</span></span>: <span class="type">Array</span>[<span class="type">Partition</span>] = &#123;</span><br><span class="line">    checkpointRDD.map(_.partitions).getOrElse &#123;</span><br><span class="line">      <span class="keyword">if</span> (partitions_ == <span class="literal">null</span>) &#123;</span><br><span class="line">        partitions_ = getPartitions</span><br><span class="line">      &#125;</span><br><span class="line">      partitions_</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>



<h5 id="RDD-分区个数分配原则"><a href="#RDD-分区个数分配原则" class="headerlink" title="RDD 分区个数分配原则"></a>RDD 分区个数分配原则</h5><p>尽可能使得分区的个数，等于集群核心数目。</p>
<p><strong>转换操作</strong>得到的 RDD 的分区个数：</p>
<ul>
<li>narrow：子 RDD 由<strong>父 RDD 分区个数</strong>决定</li>
<li>Shuffle：依赖由子 RDD <strong>分区器</strong>决定</li>
</ul>
<p><code>parallelize</code> 方法，通过 <code>defaultParallelism</code> 参数来决定分区大小。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parallelize</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    seq: <span class="type">Seq</span>[<span class="type">T</span>],</span><br><span class="line">    numSlices: <span class="type">Int</span> = defaultParallelism): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">  assertNotStopped()</span><br><span class="line">  <span class="keyword">new</span> <span class="type">ParallelCollectionRDD</span>[<span class="type">T</span>](<span class="keyword">this</span>, seq, numSlices, <span class="type">Map</span>[<span class="type">Int</span>, <span class="type">Seq</span>[<span class="type">String</span>]]())</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>defaultParallelism</code> 参数由 spark 不同的模式来确定，例如 <code>SingleCoreMockBackend</code> 只有一个 <code>core</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">SingleCoreMockBackend</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="class">  conf: <span class="type">SparkConf</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">  taskScheduler: <span class="type">TaskSchedulerImpl</span></span>) <span class="keyword">extends</span> <span class="title">MockBackend</span>(<span class="params">conf, taskScheduler</span>) </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> cores = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">defaultParallelism</span></span>(): <span class="type">Int</span> = conf.getInt(<span class="string">&quot;spark.default.parallelism&quot;</span>, cores)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p><code>textFile</code> 方法，最小是 2</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">textFile</span></span>(</span><br><span class="line">    path: <span class="type">String</span>,</span><br><span class="line">    minPartitions: <span class="type">Int</span> = defaultMinPartitions): <span class="type">RDD</span>[<span class="type">String</span>] = withScope &#123;</span><br><span class="line">  assertNotStopped()</span><br><span class="line">  hadoopFile(path, classOf[<span class="type">TextInputFormat</span>], classOf[<span class="type">LongWritable</span>], classOf[<span class="type">Text</span>],</span><br><span class="line">    minPartitions).map(pair =&gt; pair._2.toString).setName(path)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">defaultMinPartitions</span></span>: <span class="type">Int</span> = math.min(defaultParallelism, <span class="number">2</span>)</span><br></pre></td></tr></table></figure>





<h5 id="RDD-分区内记录个数"><a href="#RDD-分区内记录个数" class="headerlink" title="RDD 分区内记录个数"></a>RDD 分区内记录个数</h5><p>尽可能使同一 RDD 不同分区内的记录的数量一致。</p>
<ul>
<li>narrow：依赖于<strong>父 RDD 中相同编号分区</strong>是如何进行数据分配的</li>
<li>shuffle：依赖于选择的<strong>分区器</strong>，哈希分区器无法保证数据被平均分配到各个分区，而范围分区器则能做到这一点。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="class"><span class="keyword">object</span> <span class="title">ParallelCollectionRDD</span> </span>&#123;</span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Slice a collection into numSlices sub-collections. One extra thing we do here is to treat Range</span></span><br><span class="line"><span class="comment">   * collections specially, encoding the slices as other Ranges to minimize memory cost. This makes</span></span><br><span class="line"><span class="comment">   * it efficient to run Spark over RDDs representing large sets of numbers. And if the collection</span></span><br><span class="line"><span class="comment">   * is an inclusive Range, we use inclusive range for the last slice.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">slice</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](seq: <span class="type">Seq</span>[<span class="type">T</span>], numSlices: <span class="type">Int</span>): <span class="type">Seq</span>[<span class="type">Seq</span>[<span class="type">T</span>]] = &#123;</span><br><span class="line">    <span class="keyword">if</span> (numSlices &lt; <span class="number">1</span>) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(<span class="string">&quot;Positive number of partitions required&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// Sequences need to be sliced at the same set of index positions for operations</span></span><br><span class="line">    <span class="comment">// like RDD.zip() to behave as expected</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">positions</span></span>(length: <span class="type">Long</span>, numSlices: <span class="type">Int</span>): <span class="type">Iterator</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = &#123;</span><br><span class="line">      (<span class="number">0</span> until numSlices).iterator.map &#123; i =&gt;</span><br><span class="line">        <span class="keyword">val</span> start = ((i * length) / numSlices).toInt</span><br><span class="line">        <span class="keyword">val</span> end = (((i + <span class="number">1</span>) * length) / numSlices).toInt</span><br><span class="line">        (start, end)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    seq <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> r: <span class="type">Range</span> =&gt;</span><br><span class="line">        positions(r.length, numSlices).zipWithIndex.map &#123; <span class="keyword">case</span> ((start, end), index) =&gt;</span><br><span class="line">          <span class="comment">// If the range is inclusive, use inclusive range for the last slice</span></span><br><span class="line">          <span class="keyword">if</span> (r.isInclusive &amp;&amp; index == numSlices - <span class="number">1</span>) &#123;</span><br><span class="line">            <span class="keyword">new</span> <span class="type">Range</span>.<span class="type">Inclusive</span>(r.start + start * r.step, r.end, r.step)</span><br><span class="line">          &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">new</span> <span class="type">Range</span>.<span class="type">Inclusive</span>(r.start + start * r.step, r.start + (end - <span class="number">1</span>) * r.step, r.step)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;.toSeq.asInstanceOf[<span class="type">Seq</span>[<span class="type">Seq</span>[<span class="type">T</span>]]]</span><br><span class="line">      <span class="keyword">case</span> nr: <span class="type">NumericRange</span>[<span class="type">T</span>] =&gt;</span><br><span class="line">        <span class="comment">// For ranges of Long, Double, BigInteger, etc</span></span><br><span class="line">        <span class="keyword">val</span> slices = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">Seq</span>[<span class="type">T</span>]](numSlices)</span><br><span class="line">        <span class="keyword">var</span> r = nr</span><br><span class="line">        <span class="keyword">for</span> ((start, end) &lt;- positions(nr.length, numSlices)) &#123;</span><br><span class="line">          <span class="keyword">val</span> sliceSize = end - start</span><br><span class="line">          slices += r.take(sliceSize).asInstanceOf[<span class="type">Seq</span>[<span class="type">T</span>]]</span><br><span class="line">          r = r.drop(sliceSize)</span><br><span class="line">        &#125;</span><br><span class="line">        slices.toSeq</span><br><span class="line">      <span class="keyword">case</span> _ =&gt;</span><br><span class="line">        <span class="keyword">val</span> array = seq.toArray <span class="comment">// To prevent O(n^2) operations for List etc</span></span><br><span class="line">        positions(array.length, numSlices).map &#123; <span class="keyword">case</span> (start, end) =&gt;</span><br><span class="line">            array.slice(start, end).toSeq</span><br><span class="line">        &#125;.toSeq</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>就是构造等长的分区。</p>
<h4 id="RDD-依赖"><a href="#RDD-依赖" class="headerlink" title="RDD 依赖"></a>RDD 依赖</h4><p>在外部通常把记录的信息成为血缘关系。在内部记录则是 RDD 之间的依赖 Dependancy。</p>
<p>依赖只保存在<strong>父 RDD</strong> 中。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@DeveloperApi</span></span><br><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Dependency</span>[<span class="type">T</span>] <span class="keyword">extends</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">rdd</span></span>: <span class="type">RDD</span>[<span class="type">T</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<blockquote>
<p>依赖的分类</p>
</blockquote>
<p>依赖关系指两个 RDD 之间的依赖关系。如果一次转换中包含多个父依赖，则<strong>可能同时存在 narrow 和 wild</strong></p>
<p><img src="https://ihainan.gitbooks.io/spark-source-code/content/media/images/section1/RDDDependencies/ComplexDependencies.png"></p>
<h5 id="narrow"><a href="#narrow" class="headerlink" title="narrow"></a>narrow</h5><p>Narrow 实现在 <code>NarrowDependency</code> 中。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@DeveloperApi</span></span><br><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">NarrowDependency</span>[<span class="type">T</span>](<span class="params">_rdd: <span class="type">RDD</span>[<span class="type">T</span>]</span>) <span class="keyword">extends</span> <span class="title">Dependency</span>[<span class="type">T</span>] </span>&#123;</span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Get the parent partitions for a child partition.</span></span><br><span class="line"><span class="comment">   * 用于获取分区来源于父 RDD 中的哪一个分区，只会返回一个元素</span></span><br><span class="line"><span class="comment">   * @param partitionId a partition of the child RDD</span></span><br><span class="line"><span class="comment">   * @return the partitions of the parent RDD that the child partition depends upon</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getParents</span></span>(partitionId: <span class="type">Int</span>): <span class="type">Seq</span>[<span class="type">Int</span>]</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">rdd</span></span>: <span class="type">RDD</span>[<span class="type">T</span>] = _rdd</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p>narrow 可以进一步分为 <strong>一对一依赖</strong> 和 <strong>范围依赖</strong></p>
<blockquote>
<p>一对一依赖</p>
</blockquote>
<p>一对一依赖表示子 RDD 分区的编号与父 RDD 分区的编号完全一致的情况。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@DeveloperApi</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">OneToOneDependency</span>[<span class="type">T</span>](<span class="params">rdd: <span class="type">RDD</span>[<span class="type">T</span>]</span>) <span class="keyword">extends</span> <span class="title">NarrowDependency</span>[<span class="type">T</span>](<span class="params">rdd</span>) </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getParents</span></span>(partitionId: <span class="type">Int</span>) = <span class="type">List</span>(partitionId)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<blockquote>
<p>范围依赖</p>
</blockquote>
<p>范围依赖是子 RDD 有父 RDD，但是分区信息还是一对一的。如图：</p>
<p><img src="https://ihainan.gitbooks.io/spark-source-code/content/media/images/section1/RDDDependencies/RangeDependencyExample.png"></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * :: DeveloperApi ::</span></span><br><span class="line"><span class="comment"> * Represents a one-to-one dependency between ranges of partitions in the parent and child RDDs.</span></span><br><span class="line"><span class="comment"> * @param rdd the parent RDD</span></span><br><span class="line"><span class="comment"> * @param inStart the start of the range in the parent RDD</span></span><br><span class="line"><span class="comment"> * @param outStart the start of the range in the child RDD</span></span><br><span class="line"><span class="comment"> * @param length the length of the range</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@DeveloperApi</span></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">RangeDependency</span>[<span class="type">T</span>](<span class="params">rdd: <span class="type">RDD</span>[<span class="type">T</span>], inStart: <span class="type">Int</span>, outStart: <span class="type">Int</span>, length: <span class="type">Int</span></span>)</span></span><br><span class="line">  <span class="keyword">extends</span> <span class="type">NarrowDependency</span>[<span class="type">T</span>](rdd) &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getParents</span></span>(partitionId: <span class="type">Int</span>) = &#123;</span><br><span class="line">      <span class="comment">// 如果在子分区中，返回对应父分区</span></span><br><span class="line">    <span class="keyword">if</span> (partitionId &gt;= outStart &amp;&amp; partitionId &lt; outStart + length) &#123;</span><br><span class="line">      <span class="type">List</span>(partitionId - outStart + inStart)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// 否则是 Nil</span></span><br><span class="line">      <span class="type">Nil</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h5 id="shuffle"><a href="#shuffle" class="headerlink" title="shuffle"></a>shuffle</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * :: DeveloperApi ::</span></span><br><span class="line"><span class="comment"> * Represents a dependency on the output of a shuffle stage. Note that in the case of shuffle,</span></span><br><span class="line"><span class="comment"> * the RDD is transient since we don&#x27;t need it on the executor side.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @param _rdd the parent RDD</span></span><br><span class="line"><span class="comment"> * @param partitioner partitioner used to partition the shuffle output</span></span><br><span class="line"><span class="comment"> * @param serializer [[org.apache.spark.serializer.Serializer Serializer]] to use. If set to None,</span></span><br><span class="line"><span class="comment"> *                   the default serializer, as specified by `spark.serializer` config option, will</span></span><br><span class="line"><span class="comment"> *                   be used.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@DeveloperApi</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ShuffleDependency</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>](<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="class">    @transient _rdd: <span class="type">RDD</span>[_ &lt;: <span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]],</span></span></span><br><span class="line"><span class="params"><span class="class">val partitioner: <span class="type">Partitioner</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">val serializer: <span class="type">Option</span>[<span class="type">Serializer</span>] = <span class="type">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">    val keyOrdering: <span class="type">Option</span>[<span class="type">Ordering</span>[<span class="type">K</span>]] = <span class="type">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">    val aggregator: <span class="type">Option</span>[<span class="type">Aggregator</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>]] = <span class="type">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">    val mapSideCombine: <span class="type">Boolean</span> = false</span>)</span></span><br><span class="line">  <span class="keyword">extends</span> <span class="type">Dependency</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]] &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">rdd</span> </span>= _rdd.asInstanceOf[<span class="type">RDD</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]]]</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> shuffleId: <span class="type">Int</span> = _rdd.context.newShuffleId()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> shuffleHandle: <span class="type">ShuffleHandle</span> = _rdd.context.env.shuffleManager.registerShuffle(</span><br><span class="line">      shuffleId, _rdd.partitions.size, <span class="keyword">this</span>)</span><br><span class="line"></span><br><span class="line">  _rdd.sparkContext.cleaner.foreach(_.registerShuffleForCleanup(<span class="keyword">this</span>))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h4 id="RDD-持久化"><a href="#RDD-持久化" class="headerlink" title="RDD 持久化"></a>RDD 持久化</h4><p>Spark 速度非常快的原因之一，就是在不同操作中<strong>可以在内存中持久化或缓存个数据集</strong>。</p>
<p>缓存是 Spark 构建 <code>迭代式算法</code> 和 <code>快速交互式查询</code> 的关键。</p>
<p>如果一个有持久化数据的节点<strong>发生故障</strong>，Spark 会在需要<strong>用到缓存的数据时重算丢失的数据分区</strong>。如果希望节点故障的情况不会拖累我们的执行速度，也可以把数据<strong>备份到多个节点</strong>上。</p>
<p>两种持久化操作：</p>
<ul>
<li>persist(StorageLevel)</li>
<li>cache， cache 就相当于 MEMORY_ONLY 的 persist</li>
</ul>
<h5 id="RDD-缓存方式"><a href="#RDD-缓存方式" class="headerlink" title="RDD 缓存方式"></a>RDD 缓存方式</h5><p>[[内存管理]] 可以查看 on-heap 和 off-heap 内存的优缺点。</p>
<p>默认情况下 <code>persist()</code> 会把数据以序列化的形式缓存在 JVM 的堆空间中。</p>
<p>这两个方法并<strong>不是被调用时立即缓存</strong>，而是触发后面的 <strong>action</strong> 时，该 RDD 将会被缓存在计算节点的内存中，并供后面重用。</p>
<p>在存储级别的末尾加上 <code>_2</code> 来把持久化数据存为两份。例如 <code>DISK_ONLY_2</code></p>
<p><img src="https://s2.ax1x.com/2019/04/26/Enf9Nn.png" alt="storage level"></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StorageLevel</span> </span>&#123;  </span><br><span class="line">  <span class="keyword">val</span> <span class="type">NONE</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>)  </span><br><span class="line">  <span class="keyword">val</span> <span class="type">DISK_ONLY</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>)  </span><br><span class="line">  <span class="keyword">val</span> <span class="type">DISK_ONLY_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)  </span><br><span class="line">  <span class="keyword">val</span> <span class="type">DISK_ONLY_3</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">3</span>)  </span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>)  </span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>, <span class="number">2</span>)  </span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY_SER</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>)  </span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY_SER_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)  </span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>)  </span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>, <span class="number">2</span>)  </span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_SER</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>)  </span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_SER_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)  </span><br><span class="line">  <span class="keyword">val</span> <span class="type">OFF_HEAP</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="number">1</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="persist-cache-unpersist"><a href="#persist-cache-unpersist" class="headerlink" title="persist, cache, unpersist"></a>persist, cache, unpersist</h5><blockquote>
<p>persist and cache</p>
</blockquote>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// cache() 等同于 persist() 等同于 persist(StorageLevel.MEMORY_ONLY) ，也就是仅缓存于存储内存中。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cache</span></span>(): <span class="keyword">this</span>.<span class="keyword">type</span> = persist()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">persist</span></span>(): <span class="keyword">this</span>.<span class="keyword">type</span> = persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 缓存级别，由5个参数组成</span></span><br><span class="line"><span class="keyword">new</span> <span class="type">StorageLevel</span>(useDisk, useMemory, useOffHeap, deserialized, replication))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">persist</span></span>(newLevel: <span class="type">StorageLevel</span>): <span class="keyword">this</span>.<span class="keyword">type</span> = &#123;</span><br><span class="line">  <span class="comment">// isLocallyCheckpointed 方法 判断该RDD是否已经标记为 checkpoint，注意不是cache</span></span><br><span class="line">  <span class="keyword">if</span> (isLocallyCheckpointed) &#123;</span><br><span class="line">    persist(<span class="type">LocalRDDCheckpointData</span>.transformStorageLevel(newLevel), allowOverride = <span class="literal">true</span>)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    persist(newLevel, allowOverride = <span class="literal">false</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">persist</span></span>(newLevel: <span class="type">StorageLevel</span>, allowOverride: <span class="type">Boolean</span>): <span class="keyword">this</span>.<span class="keyword">type</span> = &#123;</span><br><span class="line">    <span class="comment">// <span class="doctag">TODO:</span> Handle changes of StorageLevel</span></span><br><span class="line">    <span class="keyword">if</span> (storageLevel != <span class="type">StorageLevel</span>.<span class="type">NONE</span> &amp;&amp; newLevel != storageLevel &amp;&amp; !allowOverride) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="type">SparkCoreErrors</span>.cannotChangeStorageLevelError()</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// If this is the first time this RDD is marked for persisting, register it</span></span><br><span class="line">    <span class="comment">// with the SparkContext for cleanups and accounting. Do this only once.</span></span><br><span class="line">    <span class="keyword">if</span> (storageLevel == <span class="type">StorageLevel</span>.<span class="type">NONE</span>) &#123;</span><br><span class="line">        sc.cleaner.foreach(_.registerRDDForCleanup(<span class="keyword">this</span>))</span><br><span class="line">        sc.persistRDD(<span class="keyword">this</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 修改 rdd 的存储级别</span></span><br><span class="line">    storageLevel = newLevel</span><br><span class="line">    <span class="keyword">this</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Register an RDD to be persisted in memory and/or disk storage</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">persistRDD</span></span>(rdd: <span class="type">RDD</span>[_]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 相当于打一个标记，真正触发 rdd storage 的地方是在 iterator 的时候</span></span><br><span class="line">    persistentRdds(rdd.id) = rdd</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<blockquote>
<p>unpersist</p>
</blockquote>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">unpersist</span></span>(blocking: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="keyword">this</span>.<span class="keyword">type</span> = &#123;</span><br><span class="line">    logInfo(<span class="string">s&quot;Removing RDD <span class="subst">$id</span> from persistence list&quot;</span>)</span><br><span class="line">    sc.unpersistRDD(id, blocking)</span><br><span class="line">    storageLevel = <span class="type">StorageLevel</span>.<span class="type">NONE</span></span><br><span class="line">    <span class="keyword">this</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">unpersistRDD</span></span>(rddId: <span class="type">Int</span>, blocking: <span class="type">Boolean</span> = <span class="literal">true</span>) &#123;</span><br><span class="line">    <span class="comment">// 通知blockManager删掉属于该RDD的全部block</span></span><br><span class="line">    env.blockManager.master.removeRdd(rddId, blocking)</span><br><span class="line">    <span class="comment">// 从map中移掉它</span></span><br><span class="line">    persistentRdds.remove(rddId)</span><br><span class="line">    listenerBus.post(<span class="type">SparkListenerUnpersistRDD</span>(rddId))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p>获取 persistent RDD 的方法：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdds = sc.getPersistentRDDs</span><br><span class="line">rdds: scala.collection.<span class="type">Map</span>[<span class="type">Int</span>,org.apache.spark.rdd.<span class="type">RDD</span>[_]] = <span class="type">Map</span>(<span class="number">5</span> -&gt; <span class="type">ShuffledRDD</span>[<span class="number">5</span>] at partitionBy at &lt;console&gt;:<span class="number">28</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; rdds.foreach(println)</span><br><span class="line">(<span class="number">5</span>,<span class="type">ShuffledRDD</span>[<span class="number">5</span>] at partitionBy at &lt;console&gt;:<span class="number">28</span>)</span><br></pre></td></tr></table></figure>

<h5 id="iterator"><a href="#iterator" class="headerlink" title="iterator"></a>iterator</h5><p><a class="link" target="_blank" rel="noopener" href="https://www.runoob.com/scala/currying-functions.html">scala curring function<i class="fas fa-external-link-alt"></i></a></p>
<p><a class="link" target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/541508600">RDD缓存的实现逻辑分析<i class="fas fa-external-link-alt"></i></a></p>
<p>iterator 是真正触发存储的地方</p>
<ol>
<li><p><code>iterator</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> <span class="function"><span class="keyword">def</span> <span class="title">iterator</span></span>(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[<span class="type">T</span>] = &#123;</span><br><span class="line">    <span class="keyword">if</span> (storageLevel != <span class="type">StorageLevel</span>.<span class="type">NONE</span>) &#123;</span><br><span class="line">       	<span class="comment">// 如果存储等级存在的话，调用</span></span><br><span class="line">        getOrCompute(split, context)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// compute or 读取 checkpoint</span></span><br><span class="line">        computeOrReadCheckpoint(split, context)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


</li>
<li><p><code>getOrComput</code> </p>
</li>
<li><ul>
<li>通过 cache 或者 checkpoint 读取数据</li>
<li>否则 comput，然后<strong>存储</strong></li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">getOrCompute</span></span>(partition: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[<span class="type">T</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> blockId = <span class="type">RDDBlockId</span>(id, partition.index)</span><br><span class="line">    <span class="keyword">var</span> readCachedBlock = <span class="literal">true</span></span><br><span class="line">    <span class="type">SparkEnv</span>.get.blockManager.getOrElseUpdate(blockId, storageLevel, elementClassTag, () =&gt; &#123;</span><br><span class="line">        <span class="comment">// 定义了如果 rdd 没有存储，计算的过程</span></span><br><span class="line">        readCachedBlock = <span class="literal">false</span></span><br><span class="line">        computeOrReadCheckpoint(partition, context)</span><br><span class="line">    &#125;) <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="comment">// Block hit. 通过 comput 或者 存储 获取到数据</span></span><br><span class="line">        <span class="keyword">case</span> <span class="type">Left</span>(blockResult) =&gt;</span><br><span class="line">        ...</span><br><span class="line">        <span class="comment">// Need to compute the block. 只有不够存储的时候才会出发这个 comput</span></span><br><span class="line">        <span class="keyword">case</span> <span class="type">Right</span>(iter) =&gt;</span><br><span class="line">        <span class="keyword">new</span> <span class="type">InterruptibleIterator</span>(context, iter)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">computeOrReadCheckpoint</span></span>(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[<span class="type">T</span>] =</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span> (isCheckpointedAndMaterialized) &#123;</span><br><span class="line">        <span class="comment">// 如果有 checkpoint，TODO</span></span><br><span class="line">        firstParent[<span class="type">T</span>].iterator(split, context)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// 否则计算</span></span><br><span class="line">        compute(split, context)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/***</span></span><br><span class="line"><span class="comment">   * :: DeveloperApi ::</span></span><br><span class="line"><span class="comment">   * Implemented by subclasses to compute a given partition.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line"><span class="meta">@DeveloperApi</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure>


</li>
<li><p><code>getOrElseUpdate</code></p>
<ul>
<li>命中，读取 storage</li>
<li>不命中，comput and storage</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getOrElseUpdate</span></span>[<span class="type">T</span>](</span><br><span class="line">    blockId: <span class="type">BlockId</span>,</span><br><span class="line">    level: <span class="type">StorageLevel</span>,</span><br><span class="line">    classTag: <span class="type">ClassTag</span>[<span class="type">T</span>],</span><br><span class="line">    makeIterator: () =&gt; <span class="type">Iterator</span>[<span class="type">T</span>]): <span class="type">Either</span>[<span class="type">BlockResult</span>, <span class="type">Iterator</span>[<span class="type">T</span>]] = &#123;</span><br><span class="line">    <span class="comment">// hit local or remote storage, retrun ans</span></span><br><span class="line">    get[<span class="type">T</span>](blockId)(classTag) <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="type">Some</span>(block) =&gt;</span><br><span class="line">        <span class="keyword">return</span> <span class="type">Left</span>(block)</span><br><span class="line">        <span class="keyword">case</span> _ =&gt;</span><br><span class="line">        <span class="comment">// Need to compute the block.</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// Initially we hold no locks on this block.</span></span><br><span class="line">    doPutIterator(blockId, makeIterator, level, classTag, keepReadLock = <span class="literal">true</span>) <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">        <span class="comment">// SUCCESS the block already existed or was successfully stored</span></span><br><span class="line">        <span class="type">Left</span>(blockResult)</span><br><span class="line">        <span class="keyword">case</span> <span class="type">Some</span>(iter) =&gt;</span><br><span class="line">        <span class="comment">// FAILED</span></span><br><span class="line">        <span class="comment">// The put failed, likely because the data was too large to fit in memory and could not be</span></span><br><span class="line">        <span class="comment">// dropped to disk. Therefore, we need to pass the input iterator back to the caller so</span></span><br><span class="line">        <span class="comment">// that they can decide what to do with the values (e.g. process them without caching).</span></span><br><span class="line">        <span class="type">Right</span>(iter)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


</li>
<li><p><code>doPutIterator</code></p>
<ol>
<li>优先使用 memory<ol>
<li>可以序列化 <code>memoryStore.putIteratorAsBytes</code></li>
<li>不能序列化 <code>memoryStore.putIteratorAsValues</code></li>
<li>使用 disk <code>diskStore.put</code></li>
</ol>
</li>
<li>使用 disk <code>diskStore.put</code></li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">doPutIterator</span></span>[<span class="type">T</span>](</span><br><span class="line">    blockId: <span class="type">BlockId</span>,</span><br><span class="line">    iterator: () =&gt; <span class="type">Iterator</span>[<span class="type">T</span>],</span><br><span class="line">    level: <span class="type">StorageLevel</span>,</span><br><span class="line">    classTag: <span class="type">ClassTag</span>[<span class="type">T</span>],</span><br><span class="line">    tellMaster: <span class="type">Boolean</span> = <span class="literal">true</span>,</span><br><span class="line">    keepReadLock: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="type">Option</span>[<span class="type">PartiallyUnrolledIterator</span>[<span class="type">T</span>]] = &#123;</span><br><span class="line">    doPut(blockId, level, classTag, tellMaster = tellMaster, keepReadLock = keepReadLock) &#123; info =&gt;</span><br><span class="line">        <span class="keyword">val</span> startTimeNs = <span class="type">System</span>.nanoTime()</span><br><span class="line">        <span class="keyword">var</span> iteratorFromFailedMemoryStorePut: <span class="type">Option</span>[<span class="type">PartiallyUnrolledIterator</span>[<span class="type">T</span>]] = <span class="type">None</span></span><br><span class="line">        <span class="comment">// Size of the block in bytes</span></span><br><span class="line">        <span class="keyword">var</span> size = <span class="number">0</span>L</span><br><span class="line">        <span class="keyword">if</span> (level.useMemory) &#123;</span><br><span class="line">            <span class="comment">// 优先使用内存</span></span><br><span class="line">            <span class="comment">// Put it in memory first, even if it also has useDisk set to true;</span></span><br><span class="line">            <span class="comment">// We will drop it to disk later if the memory store can&#x27;t hold it.</span></span><br><span class="line">            <span class="keyword">if</span> (level.deserialized) &#123;</span><br><span class="line">                <span class="comment">// 不能序列化，iterator() 需要启动 executor 先进行计算，得到 res，存储</span></span><br><span class="line">                memoryStore.putIteratorAsValues(blockId, iterator(), level.memoryMode, classTag) <span class="keyword">match</span> &#123;</span><br><span class="line">                    <span class="keyword">case</span> <span class="type">Right</span>(s) =&gt;</span><br><span class="line">                    size = s</span><br><span class="line">                    <span class="keyword">case</span> <span class="type">Left</span>(iter) =&gt;</span><br><span class="line">                    <span class="comment">// Not enough space to unroll this block; drop to disk if applicable</span></span><br><span class="line">                    <span class="keyword">if</span> (level.useDisk) &#123;</span><br><span class="line">                        <span class="comment">// 使用 disk 存储</span></span><br><span class="line">                        diskStore.put(blockId) &#123; channel =&gt;</span><br><span class="line">                            <span class="keyword">val</span> out = <span class="type">Channels</span>.newOutputStream(channel)</span><br><span class="line">                            serializerManager.dataSerializeStream(blockId, out, iter)(classTag)</span><br><span class="line">                        &#125;</span><br><span class="line">                        size = diskStore.getSize(blockId)</span><br><span class="line">                    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                        iteratorFromFailedMemoryStorePut = <span class="type">Some</span>(iter)</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123; <span class="comment">// !level.deserialized</span></span><br><span class="line">                <span class="comment">// 可以序列化，iterator() 需要启动 executor 先进行计算，得到 res，存储</span></span><br><span class="line">                memoryStore.putIteratorAsBytes(blockId, iterator(), classTag, level.memoryMode) <span class="keyword">match</span> &#123;</span><br><span class="line">                    <span class="keyword">case</span> <span class="type">Right</span>(s) =&gt;</span><br><span class="line">                    size = s</span><br><span class="line">                    <span class="keyword">case</span> <span class="type">Left</span>(partiallySerializedValues) =&gt;</span><br><span class="line">                    <span class="comment">// Not enough space to unroll this block; drop to disk if applicable</span></span><br><span class="line">                    <span class="keyword">if</span> (level.useDisk) &#123;</span><br><span class="line">                        <span class="comment">// 使用 disk</span></span><br><span class="line">                        logWarning(<span class="string">s&quot;Persisting block <span class="subst">$blockId</span> to disk instead.&quot;</span>)</span><br><span class="line">                        diskStore.put(blockId) &#123; channel =&gt;</span><br><span class="line">                            <span class="keyword">val</span> out = <span class="type">Channels</span>.newOutputStream(channel)</span><br><span class="line">                            partiallySerializedValues.finishWritingToStream(out)</span><br><span class="line">                        &#125;</span><br><span class="line">                        size = diskStore.getSize(blockId)</span><br><span class="line">                    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                        iteratorFromFailedMemoryStorePut = <span class="type">Some</span>(partiallySerializedValues.valuesIterator)</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (level.useDisk) &#123;</span><br><span class="line">            <span class="comment">// 使用 disk</span></span><br><span class="line">            diskStore.put(blockId) &#123; channel =&gt;</span><br><span class="line">                <span class="keyword">val</span> out = <span class="type">Channels</span>.newOutputStream(channel)</span><br><span class="line">                serializerManager.dataSerializeStream(blockId, out, iterator())(classTag)</span><br><span class="line">            &#125;</span><br><span class="line">            size = diskStore.getSize(blockId)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li><p>memory 底层存储的本质是调用 <code>putIterator</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">putIterator</span></span>[<span class="type">T</span>](</span><br><span class="line">    blockId: <span class="type">BlockId</span>,</span><br><span class="line">    values: <span class="type">Iterator</span>[<span class="type">T</span>],</span><br><span class="line">    classTag: <span class="type">ClassTag</span>[<span class="type">T</span>],</span><br><span class="line">    memoryMode: <span class="type">MemoryMode</span>,</span><br><span class="line">    valuesHolder: <span class="type">ValuesHolder</span>[<span class="type">T</span>]): <span class="type">Either</span>[<span class="type">Long</span>, <span class="type">Long</span>] = &#123;</span><br><span class="line">    require(!contains(blockId), <span class="string">s&quot;Block <span class="subst">$blockId</span> is already present in the MemoryStore&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Number of elements unrolled so far ｜ 当前需要存储的 elem 数量</span></span><br><span class="line">    <span class="keyword">var</span> elementsUnrolled = <span class="number">0</span></span><br><span class="line">    <span class="comment">// Whether there is still enough memory for us to continue unrolling this block ｜ 是否需要继续 rolling</span></span><br><span class="line">    <span class="keyword">var</span> keepUnrolling = <span class="literal">true</span></span><br><span class="line">    <span class="comment">// Initial per-task memory to request for unrolling blocks (bytes). ｜ 初始化申请 memo 的大小 default 1M</span></span><br><span class="line">    <span class="keyword">val</span> initialMemoryThreshold = unrollMemoryThreshold</span><br><span class="line">    <span class="comment">// How often to check whether we need to request more memory ｜ unrolling 多少个 elem 进行一次 memo 大小检查 default 16</span></span><br><span class="line">    <span class="keyword">val</span> memoryCheckPeriod = conf.get(<span class="type">UNROLL_MEMORY_CHECK_PERIOD</span>)</span><br><span class="line">    <span class="comment">// Memory currently reserved by this task for this particular unrolling operation ｜ 当前占用 memo 的大小</span></span><br><span class="line">    <span class="keyword">var</span> memoryThreshold = initialMemoryThreshold</span><br><span class="line">    <span class="comment">// Memory to request as a multiple of current vector size ｜ vector 扩容的 frac default 1.5</span></span><br><span class="line">    <span class="keyword">val</span> memoryGrowthFactor = conf.get(<span class="type">UNROLL_MEMORY_GROWTH_FACTOR</span>)</span><br><span class="line">    <span class="comment">// Keep track of unroll memory used by this particular block / putIterator() operation</span></span><br><span class="line">    <span class="keyword">var</span> unrollMemoryUsedByThisBlock = <span class="number">0</span>L</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Request enough memory to begin unrolling ｜ 申请初始化 memo</span></span><br><span class="line">    keepUnrolling =</span><br><span class="line">    reserveUnrollMemoryForThisTask(blockId, initialMemoryThreshold, memoryMode)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (!keepUnrolling) &#123;</span><br><span class="line">        logWarning(<span class="string">s&quot;Failed to reserve initial memory threshold of &quot;</span> +</span><br><span class="line">                   <span class="string">s&quot;<span class="subst">$&#123;Utils.bytesToString(initialMemoryThreshold)&#125;</span> for computing block <span class="subst">$blockId</span> in memory.&quot;</span>)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        unrollMemoryUsedByThisBlock += initialMemoryThreshold</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Unroll this block safely, checking whether we have exceeded our threshold periodically</span></span><br><span class="line">    <span class="keyword">while</span> (values.hasNext &amp;&amp; keepUnrolling) &#123;</span><br><span class="line">        <span class="comment">// 将当前元素加入 vector</span></span><br><span class="line">        valuesHolder.storeValue(values.next())</span><br><span class="line">        <span class="comment">// 如果到了一个 period，则进行一个 memo 容量检查</span></span><br><span class="line">        <span class="keyword">if</span> (elementsUnrolled % memoryCheckPeriod == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="comment">// 估算当前 vector 的存储容量</span></span><br><span class="line">            <span class="keyword">val</span> currentSize = valuesHolder.estimatedSize()</span><br><span class="line">            <span class="comment">// If our vector&#x27;s size has exceeded the threshold, request more memory ｜ 超过容量需要 memo 扩容</span></span><br><span class="line">            <span class="keyword">if</span> (currentSize &gt;= memoryThreshold) &#123;</span><br><span class="line">                <span class="keyword">val</span> amountToRequest = (currentSize * memoryGrowthFactor - memoryThreshold).toLong</span><br><span class="line">                keepUnrolling =</span><br><span class="line">                reserveUnrollMemoryForThisTask(blockId, amountToRequest, memoryMode)</span><br><span class="line">                <span class="keyword">if</span> (keepUnrolling) &#123;</span><br><span class="line">                    unrollMemoryUsedByThisBlock += amountToRequest</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">// New threshold is currentSize * memoryGrowthFactor</span></span><br><span class="line">                memoryThreshold += amountToRequest</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        elementsUnrolled += <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Make sure that we have enough memory to store the block. By this point, it is possible that</span></span><br><span class="line">    <span class="comment">// the block&#x27;s actual memory usage has exceeded the unroll memory by a small amount, so we</span></span><br><span class="line">    <span class="comment">// perform one final call to attempt to allocate additional memory if necessary.</span></span><br><span class="line">    <span class="keyword">if</span> (keepUnrolling) &#123;</span><br><span class="line">        <span class="keyword">val</span> entry = entryBuilder.build()</span><br><span class="line">        <span class="comment">// Synchronize so that transfer is atomic ｜ 进行一个 memo 的存储</span></span><br><span class="line">        entries.synchronized &#123;</span><br><span class="line">            entries.put(blockId, entry)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        logInfo(<span class="string">&quot;Block %s stored as values in memory (estimated size %s, free %s)&quot;</span>.format(blockId,</span><br><span class="line">                                                                                          <span class="type">Utils</span>.bytesToString(entry.size), 																						      <span class="type">Utils</span>.bytesToString(maxMemory - blocksMemoryUsed)))</span><br><span class="line">        <span class="type">Right</span>(entry.size)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// We ran out of space while unrolling the values for this block</span></span><br><span class="line">        logUnrollFailureMessage(blockId, valuesHolder.estimatedSize())</span><br><span class="line">        <span class="type">Left</span>(unrollMemoryUsedByThisBlock)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
<h5 id="checkpoint"><a href="#checkpoint" class="headerlink" title="checkpoint"></a>checkpoint</h5><p>cache 和 persist 将数据持久化到 memory 或者 disk，而且保存了血缘关系。如果出现 node crash 的情况还是可以重新计算的。</p>
<p>chechpoint 是直接将数据持久化到 hdfs 中，因为 hdfs 的高可靠性，所以所以阶段之前的血缘关系。</p>
<blockquote>
<p>checkpoint saves the RDD to an HDFS file and actually forgets the lineage completely. This is allows long lineages to be truncated and the data to be saved reliably in HDFS</p>
</blockquote>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 设置 checkpoint 的保存路径</span></span><br><span class="line">sc.setCheckpointDir(<span class="string">&quot;ckp&quot;</span>)</span><br></pre></td></tr></table></figure>



<h3 id="环境搭建"><a href="#环境搭建" class="headerlink" title="环境搭建"></a>环境搭建</h3><p>分析 <code>spark-shubmit</code> 的执行流程：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$</span><span class="language-bash">SPARK_HOME/bin/spark-submit \</span></span><br><span class="line"><span class="language-bash">--class <span class="string">&quot;SimpleApp&quot;</span> \</span></span><br><span class="line"><span class="language-bash">--master <span class="built_in">local</span> \ <span class="comment"># 5.3 构建 app 对象 （STANDALONE_CLUSTER_SUBMIT_CLASS）</span></span></span><br><span class="line">./target/scala-2.13/simple-app_2.13-1.0.jar</span><br></pre></td></tr></table></figure>


<ol>
<li><p>调用 spark-submit 脚本会执行 <code>java org.apache.deploy.SparkSubmit</code> 对象。</p>
</li>
<li><p>创建 SparkSubmit 对象调用 doSubmit 方法</p>
</li>
<li><p>解析命令行参数，默认 <code>action</code> 为 <code>SUBMIT</code>, <code>action = Option(action).getOrElse(SUBMIT)</code></p>
</li>
<li><p>调用 submit 方法，&#x3D;&gt; 调用 runMain 方法。</p>
</li>
<li><p>runMain 方法</p>
</li>
<li><p>解析命令行参数 &#x3D;&gt; master &#x3D;&#x3D; ‘local’ &#x3D;&gt; LOCAL</p>
</li>
<li><p>获取 childMainClass</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[deploy] <span class="keyword">val</span> <span class="type">YARN_CLUSTER_SUBMIT_CLASS</span> =</span><br><span class="line"><span class="string">&quot;org.apache.spark.deploy.yarn.YarnClusterApplication&quot;</span></span><br><span class="line"><span class="keyword">private</span>[deploy] <span class="keyword">val</span> <span class="type">REST_CLUSTER_SUBMIT_CLASS</span> = classOf[<span class="type">RestSubmissionClientApp</span>].getName()</span><br><span class="line"><span class="keyword">private</span>[deploy] <span class="keyword">val</span> <span class="type">STANDALONE_CLUSTER_SUBMIT_CLASS</span> = classOf[<span class="type">ClientApp</span>].getName()</span><br><span class="line"><span class="keyword">private</span>[deploy] <span class="keyword">val</span> <span class="type">KUBERNETES_CLUSTER_SUBMIT_CLASS</span> =</span><br><span class="line"><span class="string">&quot;org.apache.spark.deploy.k8s.submit.KubernetesClientApplication&quot;</span></span><br></pre></td></tr></table></figure></li>
<li><p>通过 mainClass 反射创建 app，（反射：通过 classname 动态加载和构造对象），并调用 start 方法，开始运行用户的代码</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mainClass = <span class="type">Utils</span>.classForName(childMainClass)</span><br><span class="line"><span class="keyword">val</span> app: <span class="type">SparkApplication</span> = <span class="keyword">if</span> (classOf[<span class="type">SparkApplication</span>].isAssignableFrom(mainClass)) &#123;</span><br><span class="line">    mainClass.getConstructor().newInstance().asInstanceOf[<span class="type">SparkApplication</span>]</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">JavaMainApplication</span>(mainClass)</span><br><span class="line">&#125;</span><br><span class="line">app.start(childArgs.toArray, sparkConf)</span><br></pre></td></tr></table></figure>
</li>
<li><p>Driver 首先创建 sparkContext 对象的创建</p>
</li>
<li><p>当 sparkContext 初始化之后，会等待 system 资源创建完成，调用 Hook 函数等待。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Post init</span></span><br><span class="line">_taskScheduler.postStartHook()</span><br><span class="line"></span><br><span class="line"><span class="comment">// Invoked after system has successfully initialized (typically in spark context).</span></span><br><span class="line"><span class="comment">// Yarn uses this to bootstrap allocation of resources based on preferred locations,</span></span><br><span class="line"><span class="comment">// wait for executor registrations, etc.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">postStartHook</span></span>(): <span class="type">Unit</span> = &#123; &#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建完成之后，继续执行用户的代码，读写文件或者操作 rdd。</p>
</li>
</ol>
<p><img src="https://img-blog.csdn.net/20180813202155856?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3l1bWluZ3podTE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="spark-process"></p>
<p><a class="link" target="_blank" rel="noopener" href="https://blog.csdn.net/yumingzhu1/article/details/81636408?spm=1001.2101.3001.6650.2&utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-2-81636408-blog-81703467.pc_relevant_recovery_v2&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-2-81636408-blog-81703467.pc_relevant_recovery_v2&utm_relevant_index=3">standalone process<i class="fas fa-external-link-alt"></i></a></p>
<h3 id="sparkcontext"><a href="#sparkcontext" class="headerlink" title="sparkcontext"></a>sparkcontext</h3><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>在Spark框架中，应用程序的提交离不开<code>Spark Driver</code>，而<code>Spark Driver</code>的初始化始终围绕<code>SparkContext</code>的初始化，可以说<code>SparkContext</code>是<code>Spark</code>程序的发动机引擎，有了它程序才能跑起来，在<code>spark-core</code>中，<code>SparkContext</code>重中之重，它提供了很多能力，比如生成<code>RDD</code>，比如生成广播变量等，所以学习<code>SparkContext</code>的组件和启动流程有助于剖析整个<code>Spark</code>内核的架构。</p>
<h4 id="SparkContext组件概览"><a href="#SparkContext组件概览" class="headerlink" title="SparkContext组件概览"></a>SparkContext组件概览</h4><p>在SparkContext中包含了整个框架中很重要的几部分：</p>
<ul>
<li>SparkEnv：Spark的运行环境，Executor会依赖它去执行分配的task，不光Executor中有，同时为了保证本地模式任务也能跑起来，Driver中也有</li>
<li>SparkUI：Spark作业的监控页面，底层并没有采用前端技术，纯后端实现，用以对当前SparkJob的监控和调优，可以从页面观察到目前的Executor的jvm信息，每个job的stage划分和task划分，同时还可以观察到每个task处理的数据，用以发现数据是否倾斜</li>
<li>DAGScheduler：DAG调度器，是SparkJob调度系统的重要组件之一，负责创建job，根据RDD依赖情况划分stage，提交stage，将作业划分成一个有向无环图</li>
<li>TaskScheduler：任务调度器，是SparkJob调度系统的重要组件之一，负责按照调度算法将DAGScheduler创建的task分发至Executor，DAGScheduler是它的前置调度</li>
<li>SparkStatusTracker：提供对作业、Stage的监控</li>
<li>ConsoleProcessBar：利用SparkStatusTracker提供监控信息，将任务进度以日志的形式打印到终端中</li>
<li>HearbeatReceiver：心跳接收器，所有Executor都会定期向它发送心跳信息，用以统计存活的Executor，此信息会一直同步给TaskScheduler，用以保证TaskScheduler去分发task的时候会挑选合适的Executor</li>
<li>ContextCleaner：上下文清理器，用异步的方式去清理那些超出应用作用域范围的RDD、ShuffleDependency和Broadcast</li>
<li>LiveListenerBus：SparkContext中的事件总线，可以接收各个组件的事件，并且通过异步的方式对事件进行匹配并调用不同的回调方法</li>
<li>ShutdownHookManager：关闭时的钩子管理器，用以做一些清理工作，比如资源释放等</li>
<li>AppStatusStore：存储Application状态数据，在2.3.0之后的版本引入</li>
<li>EventLoggingListener（可选）：将事件持久化到存储的监听器，通过<code>spark.eventLog.enabled</code> 进行控制</li>
<li>ExecutorAllocationManager（可选）：Executor动态分配管理器，根据工作负载状态动态调整Executor的数量，通过属性<code>spark.dynamicAllocation.enabled</code> 和<code>spark.dynamicAllocation.testing</code> 进行控制</li>
</ul>
<h3 id="任务调度"><a href="#任务调度" class="headerlink" title="任务调度"></a>任务调度</h3><p><img src="https://oss-emcsprod-public.modb.pro/wechatSpider/modb_20220704_70bbb896-fb8b-11ec-a026-fa163eb4f6be.png" alt="process"></p>
<h4 id="Action-触发-Job-submit-到-DAGScheduler-eventProcessLoop"><a href="#Action-触发-Job-submit-到-DAGScheduler-eventProcessLoop" class="headerlink" title="Action 触发 Job submit 到 DAGScheduler eventProcessLoop"></a>Action 触发 Job submit 到 DAGScheduler eventProcessLoop</h4><ol>
<li><p>使用 RDD 的 collect 方法可以出发 action，导致作业提交。</p>
<p>runJob with param <code>rdd</code> <code>func</code> <code>partitions</code></p>
</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collect</span></span>(): <span class="type">Array</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">    <span class="keyword">val</span> results = sc.runJob(<span class="keyword">this</span>, (iter: <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; iter.toArray)</span><br><span class="line">    <span class="type">Array</span>.concat(results: _*)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<ol start="2">
<li><p>通过 <code>dagScheduler</code> 进行作业提交</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)</span><br></pre></td></tr></table></figure>

<p>然后通过 <code>submitJob</code> 将作业提交。并等待作业返回。</p>
</li>
<li><p><code>submitJob</code> 将 Job 加入到 dagScheduler 的事件循环中，等待 Job 被调度。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="keyword">val</span> eventProcessLoop = <span class="keyword">new</span> <span class="type">DAGSchedulerEventProcessLoop</span>(<span class="keyword">this</span>)</span><br><span class="line"></span><br><span class="line">eventProcessLoop.post(<span class="type">JobSubmitted</span>(</span><br><span class="line">      jobId, rdd, func2, partitions.toArray, callSite, waiter,</span><br><span class="line">      <span class="type">Utils</span>.cloneProperties(properties)))</span><br></pre></td></tr></table></figure></li>
</ol>
<h4 id="DAGScheduler-对-Job-进行-Stage-划分，并且提交-Task-到-TaskScheduler"><a href="#DAGScheduler-对-Job-进行-Stage-划分，并且提交-Task-到-TaskScheduler" class="headerlink" title="DAGScheduler 对 Job 进行 Stage 划分，并且提交 Task 到 TaskScheduler"></a>DAGScheduler 对 Job 进行 Stage 划分，并且提交 Task 到 TaskScheduler</h4><ol>
<li><p>dagScheduler eventProcessLoop 通过 onReceive 方法处理事件。然后调用 <code>dagScheduler.handleJobSubmitted</code> 来处理之前 Job 的提交。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * The main event loop of the DAG scheduler.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onReceive</span></span>(event: <span class="type">DAGSchedulerEvent</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> timerContext = timer.time()</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        doOnReceive(event)</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        timerContext.stop()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">doOnReceive</span></span>(event: <span class="type">DAGSchedulerEvent</span>): <span class="type">Unit</span> = event <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">JobSubmitted</span>(jobId, rdd, func, partitions, callSite, listener, properties) =&gt;</span><br><span class="line">    dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="type">MapStageSubmitted</span>(jobId, dependency, callSite, listener, properties) =&gt;</span><br><span class="line">    dagScheduler.handleMapStageSubmitted(jobId, dependency, callSite, listener, properties)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


</li>
<li><p>在 <code>handleJobSubmitted</code> 中，会进行 stage 的划分，并且将所有的 stage 包括 parent stage and ancient stage 全部提交，通过 DFS 的方式</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1. 调用创建 ResultStage 的方法</span></span><br><span class="line">finalStage = createResultStage(finalRDD, func, partitions, jobId, callSite)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1.1 首先会获取 trigger action rdd 的所有 shuffleDeps，然后使用 shuffleDeps 创建当前 rdd 的所有 parent（不包括 parent 的 parent），最后创建 resultStage</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取 shuffleDeps 的方法就是 BFS</span></span><br><span class="line"><span class="keyword">val</span> (shuffleDeps, resourceProfiles) = getShuffleDependenciesAndResourceProfiles(rdd)</span><br><span class="line"><span class="keyword">val</span> parents = getOrCreateParentStages(shuffleDeps, jobId)</span><br><span class="line"><span class="keyword">val</span> stage = <span class="keyword">new</span> <span class="type">ResultStage</span>(id, rdd, func, partitions, parents, jobId,</span><br><span class="line">                            callSite, resourceProfile.id)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 2. 提交 stage，这里的 finalStage 就是 resultStage</span></span><br><span class="line">submitStage(finalStage)</span><br></pre></td></tr></table></figure>



<p><code>getShuffleDependenciesAndResourceProfiles</code> 方法（BFS）：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[scheduler] <span class="function"><span class="keyword">def</span> <span class="title">getShuffleDependenciesAndResourceProfiles</span></span>(</span><br><span class="line">    rdd: <span class="type">RDD</span>[_]): (<span class="type">HashSet</span>[<span class="type">ShuffleDependency</span>[_, _, _]], <span class="type">HashSet</span>[<span class="type">ResourceProfile</span>]) = &#123;</span><br><span class="line">    <span class="comment">// rdd 全部 parent 列表</span></span><br><span class="line">    <span class="keyword">val</span> parents = <span class="keyword">new</span> <span class="type">HashSet</span>[<span class="type">ShuffleDependency</span>[_, _, _]]</span><br><span class="line">	<span class="comment">// 记忆化</span></span><br><span class="line">    <span class="keyword">val</span> visited = <span class="keyword">new</span> <span class="type">HashSet</span>[<span class="type">RDD</span>[_]]</span><br><span class="line">    <span class="comment">// queue</span></span><br><span class="line">    <span class="keyword">val</span> waitingForVisit = <span class="keyword">new</span> <span class="type">ListBuffer</span>[<span class="type">RDD</span>[_]]</span><br><span class="line">    waitingForVisit += rdd</span><br><span class="line">    <span class="keyword">while</span> (waitingForVisit.nonEmpty) &#123;</span><br><span class="line">        <span class="keyword">val</span> toVisit = waitingForVisit.remove(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">if</span> (!visited(toVisit)) &#123;</span><br><span class="line">            visited += toVisit</span><br><span class="line">            <span class="type">Option</span>(toVisit.getResourceProfile).foreach(resourceProfiles += _)</span><br><span class="line">            toVisit.dependencies.foreach &#123;</span><br><span class="line">                <span class="comment">// 如果是 shuffleDeps 将它加入 parent 中，stage 会从这个地方断开</span></span><br><span class="line">                <span class="keyword">case</span> shuffleDep: <span class="type">ShuffleDependency</span>[_, _, _] =&gt;</span><br><span class="line">                parents += shuffleDep</span><br><span class="line">                <span class="comment">// 如果是 narrowDeps，则将它加入 queue 进行 BFS</span></span><br><span class="line">                <span class="keyword">case</span> dependency =&gt;</span><br><span class="line">                waitingForVisit.prepend(dependency.rdd)</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    (parents, resourceProfiles)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** Submits stage, but first recursively submits any missing parents. */</span></span><br><span class="line"><span class="comment">// 提交 resultStage 和 shuffleMapStage recursively 通过 submitStage</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">submitStage</span></span>(stage: <span class="type">Stage</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> jobId = activeJobForStage(stage)</span><br><span class="line">    <span class="keyword">if</span> (jobId.isDefined) &#123;</span><br><span class="line">        logDebug(<span class="string">s&quot;submitStage(<span class="subst">$stage</span> (name=<span class="subst">$&#123;stage.name&#125;</span>;&quot;</span> +</span><br><span class="line">                 <span class="string">s&quot;jobs=<span class="subst">$&#123;stage.jobIds.toSeq.sorted.mkString(&quot;,&quot;)&#125;</span>))&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> (!waitingStages(stage) &amp;&amp; !runningStages(stage) &amp;&amp; !failedStages(stage)) &#123;</span><br><span class="line">            <span class="keyword">val</span> missing = getMissingParentStages(stage).sortBy(_.id)</span><br><span class="line">            logDebug(<span class="string">&quot;missing: &quot;</span> + missing)</span><br><span class="line">            <span class="keyword">if</span> (missing.isEmpty) &#123;</span><br><span class="line">                logInfo(<span class="string">&quot;Submitting &quot;</span> + stage + <span class="string">&quot; (&quot;</span> + stage.rdd + <span class="string">&quot;), which has no missing parents&quot;</span>)</span><br><span class="line">                <span class="comment">// 如果没有 missing 的 parent stage 就会提交当前阶段</span></span><br><span class="line">                submitMissingTasks(stage, jobId.get)</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">for</span> (parent &lt;- missing) &#123;</span><br><span class="line">                    <span class="comment">// 不然先提交 parent stage</span></span><br><span class="line">                    submitStage(parent)</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">// 最后将当前 stage 加入到 waitingStage 中</span></span><br><span class="line">                waitingStages += stage</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        abortStage(stage, <span class="string">&quot;No active job for stage &quot;</span> + stage.id, <span class="type">None</span>)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// submitMissingTasks 中会根据当前 stage 类型，然后通过 partition，rdd 创建对应的 Tasks。rdd 中有多少个分区就会创建多少个 Tasks</span></span><br><span class="line"><span class="keyword">val</span> tasks: <span class="type">Seq</span>[<span class="type">Task</span>[_]] = <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">val</span> serializedTaskMetrics = closureSerializer.serialize(stage.latestInfo.taskMetrics).array()</span><br><span class="line">    stage <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> stage: <span class="type">ShuffleMapStage</span> =&gt;</span><br><span class="line">        stage.pendingPartitions.clear()</span><br><span class="line">        partitionsToCompute.map &#123; id =&gt;</span><br><span class="line">            <span class="keyword">val</span> locs = taskIdToLocations(id)</span><br><span class="line">            <span class="keyword">val</span> part = partitions(id)</span><br><span class="line">            stage.pendingPartitions += id</span><br><span class="line">            <span class="keyword">new</span> <span class="type">ShuffleMapTask</span>(stage.id, stage.latestInfo.attemptNumber,</span><br><span class="line">                               taskBinary, part, locs, properties, serializedTaskMetrics, <span class="type">Option</span>(jobId),</span><br><span class="line">                               <span class="type">Option</span>(sc.applicationId), sc.applicationAttemptId, stage.rdd.isBarrier())</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">case</span> stage: <span class="type">ResultStage</span> =&gt;</span><br><span class="line">        partitionsToCompute.map &#123; id =&gt;</span><br><span class="line">            <span class="keyword">val</span> p: <span class="type">Int</span> = stage.partitions(id)</span><br><span class="line">            <span class="keyword">val</span> part = partitions(p)</span><br><span class="line">            <span class="keyword">val</span> locs = taskIdToLocations(id)</span><br><span class="line">            <span class="keyword">new</span> <span class="type">ResultTask</span>(stage.id, stage.latestInfo.attemptNumber,</span><br><span class="line">                           taskBinary, part, locs, id, properties, serializedTaskMetrics,</span><br><span class="line">                           <span class="type">Option</span>(jobId), <span class="type">Option</span>(sc.applicationId), sc.applicationAttemptId,</span><br><span class="line">                           stage.rdd.isBarrier())</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt;</span><br><span class="line">    abortStage(stage, <span class="string">s&quot;Task creation failed: <span class="subst">$e</span>\n<span class="subst">$&#123;Utils.exceptionString(e)&#125;</span>&quot;</span>, <span class="type">Some</span>(e))</span><br><span class="line">    runningStages -= stage</span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 最后在 taskScheduler 中提交 TaskSet</span></span><br><span class="line">taskScheduler.submitTasks(<span class="keyword">new</span> <span class="type">TaskSet</span>(</span><br><span class="line">    tasks.toArray, stage.id, stage.latestInfo.attemptNumber, jobId, properties,</span><br><span class="line">    stage.resourceProfileId))</span><br></pre></td></tr></table></figure></li>
</ol>
<h4 id="TaskScheduler-将-TaskSet-通过-backend-的调度分发到不同的-executor-上面"><a href="#TaskScheduler-将-TaskSet-通过-backend-的调度分发到不同的-executor-上面" class="headerlink" title="TaskScheduler 将 TaskSet 通过 backend 的调度分发到不同的 executor 上面"></a>TaskScheduler 将 TaskSet 通过 backend 的调度分发到不同的 executor 上面</h4><p><code>TaskScheduler</code> 将 TaskSet 进一步包装成 TaskSetManager。然后提交到 <code>schedulableBuilder</code> 中。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1. 通过 TaskSet 创建 TaskSetManager 对象</span></span><br><span class="line"><span class="keyword">val</span> manager = createTaskSetManager(taskSet, maxTaskFailures)</span><br><span class="line"><span class="comment">// 2. 将 TaskSetManager 提交到任务调度器 SchedulerBuilder</span></span><br><span class="line">schedulableBuilder.addTaskSetManager(manager, manager.taskSet.properties)</span><br><span class="line"><span class="comment">// 3. 调用 scheduler backend 处理 Task</span></span><br><span class="line">backend.reviveOffers()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3.1 LocalSchedulerBackend 会向 localEndPoint 发送一个 ReviveOffers 信号</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">var</span> localEndpoint: <span class="type">RpcEndpointRef</span> = <span class="literal">null</span></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reviveOffers</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    localEndpoint.send(<span class="type">ReviveOffers</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3.2 localEndPoint 收到消息后会通过 recevie 来判断消息，并且调用对应的函数</span></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">receive</span></span>: <span class="type">PartialFunction</span>[<span class="type">Any</span>, <span class="type">Unit</span>] = &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">ReviveOffers</span> =&gt;</span><br><span class="line">    reviveOffers()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="type">KillTask</span>(taskId, interruptThread, reason) =&gt;</span><br><span class="line">    executor.killTask(taskId, interruptThread, reason)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reviveOffers</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// local mode doesn&#x27;t support extra resources like GPUs right now</span></span><br><span class="line">    <span class="keyword">val</span> offers = <span class="type">IndexedSeq</span>(<span class="keyword">new</span> <span class="type">WorkerOffer</span>(localExecutorId, localExecutorHostname, freeCores,</span><br><span class="line">                                            <span class="type">Some</span>(rpcEnv.address.hostPort)))</span><br><span class="line">    <span class="comment">// 首先通过 scheduler 进行一个 Task 的调度。将任务放到最合适的位置</span></span><br><span class="line">    <span class="keyword">for</span> (task &lt;- scheduler.resourceOffers(offers, <span class="literal">true</span>).flatten) &#123;</span><br><span class="line">        freeCores -= scheduler.<span class="type">CPUS_PER_TASK</span></span><br><span class="line">        <span class="comment">// 然后向 executor 发送一个 launchTask 的消息</span></span><br><span class="line">        executor.launchTask(executorBackend, task)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3.2.1 scheduler 任务调度，通过 scheduler 的调度算法进行控制</span></span><br><span class="line"><span class="keyword">val</span> sortedTaskSets = rootPool.getSortedTaskSetQueue</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> sortedSchedulableQueue = schedulableQueue.asScala.toSeq.sortWith(taskSetSchedulingAlgorithm.comparator)</span><br><span class="line"></span><br><span class="line"><span class="comment">// taskSetSchedulingAlgorithm 可以是 FIFO 或者 Fair</span></span><br></pre></td></tr></table></figure>

<p><code>SchedulerBuilder</code> 是 Spark 的任务调度器，提供两种调度方式：</p>
<ul>
<li>FIFOSchedulableBuilder</li>
<li>FairSchedulableBuilder</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">trait</span> <span class="title">SchedulableBuilder</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">rootPool</span></span>: <span class="type">Pool</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">buildPools</span></span>(): <span class="type">Unit</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">addTaskSetManager</span></span>(manager: <span class="type">Schedulable</span>, properties: <span class="type">Properties</span>): <span class="type">Unit</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">FIFOSchedulableBuilder</span>(<span class="params">val rootPool: <span class="type">Pool</span></span>)</span></span><br><span class="line">  <span class="keyword">extends</span> <span class="type">SchedulableBuilder</span> <span class="keyword">with</span> <span class="type">Logging</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">FairSchedulableBuilder</span>(<span class="params">val rootPool: <span class="type">Pool</span>, sc: <span class="type">SparkContext</span></span>)</span></span><br><span class="line">  <span class="keyword">extends</span> <span class="type">SchedulableBuilder</span> <span class="keyword">with</span> <span class="type">Logging</span></span><br></pre></td></tr></table></figure>



<p>之后 exector 使用一个 Thread 执行对应的 Task</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">launchTask</span></span>(context: <span class="type">ExecutorBackend</span>, taskDescription: <span class="type">TaskDescription</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> taskId = taskDescription.taskId</span><br><span class="line">    <span class="keyword">val</span> tr = createTaskRunner(context, taskDescription)</span><br><span class="line">    runningTasks.put(taskId, tr)</span><br><span class="line">    <span class="keyword">val</span> killMark = killMarks.get(taskId)</span><br><span class="line">    <span class="keyword">if</span> (killMark != <span class="literal">null</span>) &#123;</span><br><span class="line">        tr.kill(killMark._1, killMark._2)</span><br><span class="line">        killMarks.remove(taskId)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 从线程池中取出一个线程执行 Task</span></span><br><span class="line">    threadPool.execute(tr)</span><br><span class="line">    <span class="keyword">if</span> (decommissioned) &#123;</span><br><span class="line">        log.error(<span class="string">s&quot;Launching a task while in decommissioned state.&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="shuffle-1"><a href="#shuffle-1" class="headerlink" title="shuffle"></a>shuffle</h3><p>shuffle 一定会落盘</p>
<ul>
<li>减少落盘数据量</li>
</ul>
<p><a class="link" target="_blank" rel="noopener" href="https://www.jianshu.com/p/542b243d24e9">shuffle 原理和演进<i class="fas fa-external-link-alt"></i></a></p>
<p><a class="link" target="_blank" rel="noopener" href="https://blog.csdn.net/Winner941112/article/details/82900353">预聚合<i class="fas fa-external-link-alt"></i></a>：</p>
<p>map-side预聚合之后，每个节点本地就<strong>只会有一条相同的key</strong>，因为多条相同的key都被聚合起来了。其它节点在拉取所有节点上的相同key时，就会大大减少需要拉取的数据数量，从而也就减少了磁盘IO以及网络传输开销。</p>
<ul>
<li>reduceByKey和aggregateByKey算子都会使用用户自定义的函数对每个节点本地的相同key进行预聚合。</li>
<li>groupByKey算子是不会进行预聚合的，全量的数据会在集群的各个节点之间分发和传输，性能相对来说比较差。</li>
</ul>
<h4 id="源码部分"><a href="#源码部分" class="headerlink" title="源码部分"></a>源码部分</h4><ol>
<li><p>DAGScheduler 中生成 task 的时候，会根据 stage 来生成不同的 task</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">stage <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> stage: <span class="type">ShuffleMapStage</span> =&gt;</span><br><span class="line">    stage.pendingPartitions.clear()</span><br><span class="line">    partitionsToCompute.map &#123; id =&gt;</span><br><span class="line">        <span class="comment">// ShuffleMapStage =&gt; ShuffleMapTask</span></span><br><span class="line">        <span class="keyword">new</span> <span class="type">ShuffleMapTask</span>(stage.id, stage.latestInfo.attemptNumber,</span><br><span class="line">                           taskBinary, part, locs, properties, serializedTaskMetrics, <span class="type">Option</span>(jobId),</span><br><span class="line">                           <span class="type">Option</span>(sc.applicationId), sc.applicationAttemptId, stage.rdd.isBarrier())</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> stage: <span class="type">ResultStage</span> =&gt;</span><br><span class="line">    partitionsToCompute.map &#123; id =&gt;</span><br><span class="line">        <span class="comment">// ResultStage =&gt; ResultTask</span></span><br><span class="line">        <span class="keyword">new</span> <span class="type">ResultTask</span>(stage.id, stage.latestInfo.attemptNumber,</span><br><span class="line">                       taskBinary, part, locs, id, properties, serializedTaskMetrics,</span><br><span class="line">                       <span class="type">Option</span>(jobId), <span class="type">Option</span>(sc.applicationId), sc.applicationAttemptId,</span><br><span class="line">                       stage.rdd.isBarrier())</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>「map」ShuffleTask 在 runTask 的末尾会进行数据的落盘操作，不同的 shuffleWriterProcessor 会产生不同的文件。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">dep.shuffleWriterProcessor.write(rdd, dep, mapId, context, partition)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 然后通过 shuffleHandle 通知写文件</span></span><br><span class="line">writer = manager.getWriter[<span class="type">Any</span>, <span class="type">Any</span>](</span><br><span class="line">    dep.shuffleHandle,</span><br><span class="line">    mapId,</span><br><span class="line">    context,</span><br><span class="line">    createMetricsReporter(context))</span><br><span class="line">writer.write(</span><br><span class="line">    rdd.iterator(partition, context).asInstanceOf[<span class="type">Iterator</span>[_ &lt;: <span class="type">Product2</span>[<span class="type">Any</span>, <span class="type">Any</span>]]])</span><br></pre></td></tr></table></figure>
</li>
<li><p>「reduce」ResultTask 在 runTask 中读取 map 产生的文件</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// resultMapStage 的 runTask 方法</span></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">runTask</span></span>(context: <span class="type">TaskContext</span>): <span class="type">U</span> = &#123;</span><br><span class="line">    <span class="comment">// Deserialize the RDD and the func using the broadcast variables.</span></span><br><span class="line">    <span class="keyword">val</span> threadMXBean = <span class="type">ManagementFactory</span>.getThreadMXBean</span><br><span class="line">    <span class="keyword">val</span> deserializeStartTimeNs = <span class="type">System</span>.nanoTime()</span><br><span class="line">    <span class="keyword">val</span> deserializeStartCpuTime = <span class="keyword">if</span> (threadMXBean.isCurrentThreadCpuTimeSupported) &#123;</span><br><span class="line">        threadMXBean.getCurrentThreadCpuTime</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="number">0</span>L</span><br><span class="line">    <span class="keyword">val</span> ser = <span class="type">SparkEnv</span>.get.closureSerializer.newInstance()</span><br><span class="line">    <span class="keyword">val</span> (rdd, func) = ser.deserialize[(<span class="type">RDD</span>[<span class="type">T</span>], (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">U</span>)](</span><br><span class="line">        <span class="type">ByteBuffer</span>.wrap(taskBinary.value), <span class="type">Thread</span>.currentThread.getContextClassLoader)</span><br><span class="line">    _executorDeserializeTimeNs = <span class="type">System</span>.nanoTime() - deserializeStartTimeNs</span><br><span class="line">    _executorDeserializeCpuTime = <span class="keyword">if</span> (threadMXBean.isCurrentThreadCpuTimeSupported) &#123;</span><br><span class="line">        threadMXBean.getCurrentThreadCpuTime - deserializeStartCpuTime</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="number">0</span>L</span><br><span class="line"></span><br><span class="line">    func(context, rdd.iterator(partition, context))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// shuffleRDD iterator 的 comput 方法就会读取 map 产生的文件</span></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">C</span>)] = &#123;</span><br><span class="line">    <span class="keyword">val</span> dep = dependencies.head.asInstanceOf[<span class="type">ShuffleDependency</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>]]</span><br><span class="line">    <span class="keyword">val</span> metrics = context.taskMetrics().createTempShuffleReadMetrics()</span><br><span class="line">    <span class="type">SparkEnv</span>.get.shuffleManager.getReader(</span><br><span class="line">        dep.shuffleHandle, split.index, split.index + <span class="number">1</span>, context, metrics)</span><br><span class="line">    .read()</span><br><span class="line">    .asInstanceOf[<span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">C</span>)]]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>

        </div>

        
            <div class="post-copyright-info">
                <div class="article-copyright-info-container">
    <ul class="copyright-info-content">
        <li>Post title：spark</li>
        <li>Post author：auggie</li>
        <li>Create time：2022-11-09 09:40:26</li>
        <li>
            Post link：https://ruanjiancheng.github.io/2022/11/09/spark/
        </li>
        <li>
            Copyright Notice：All articles in this blog are licensed under <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">BY-NC-SA</a> unless stating additionally.
        </li>
    </ul>
</div>

            </div>
        

        
            <ul class="post-tags-box">
                
                    <li class="tag-item">
                        <a href="/tags/tech/">#tech</a>&nbsp;
                    </li>
                
            </ul>
        

        
            <div class="article-nav">
                
                    <div class="article-prev">
                        <a class="prev"
                           rel="prev"
                           href="/2022/12/04/images/image-20220417153331430.png/"
                        >
                            <span class="left arrow-icon flex-center">
                              <i class="fas fa-chevron-left"></i>
                            </span>
                            <span class="title flex-center">
                                <span class="post-nav-title-item"></span>
                                <span class="post-nav-item">Prev posts</span>
                            </span>
                        </a>
                    </div>
                
                
                    <div class="article-next">
                        <a class="next"
                           rel="next"
                           href="/2022/11/07/scala/"
                        >
                            <span class="title flex-center">
                                <span class="post-nav-title-item">scala</span>
                                <span class="post-nav-item">Next posts</span>
                            </span>
                            <span class="right arrow-icon flex-center">
                              <i class="fas fa-chevron-right"></i>
                            </span>
                        </a>
                    </div>
                
            </div>
        

        
    </div>
</div>


                
            </div>

        </div>

        <div class="page-main-content-bottom">
            
<footer class="footer">
    <div class="info-container">
        <div class="copyright-info info-item">
            &copy;
            
                <span>2021</span> -
            
            2022
            
                &nbsp;<i class="fas fa-heart icon-animate"></i>
                &nbsp;<a href="/">auggie</a>
            
        </div>
        
            <script async 
                    src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
            <div class="website-count info-item">
                
                    Visitor Count&nbsp;<span id="busuanzi_value_site_uv"></span>&ensp;
                
                
            </div>
        
        <div class="theme-info info-item">
            Powered by <a target="_blank" href="https://hexo.io">Hexo</a>&nbsp;|&nbsp;Theme
            &nbsp;
            <a class="theme-version" target="_blank" href="https://github.com/XPoet/hexo-theme-keep">Keep v3.4.9</a>
        </div>
        
        
    </div>
</footer>

        </div>
    </div>

    
        <div class="post-tools">
            <div class="post-tools-container">
    <ul class="tools-list">
        <!-- TOC aside toggle -->
        
            <li class="tools-item page-aside-toggle">
                <i class="fas fa-outdent"></i>
            </li>
        

        <!-- go comment -->
        
    </ul>
</div>

        </div>
    

    <div class="right-bottom-side-tools">
        <div class="side-tools-container">
    <ul class="side-tools-list">
        <li class="tools-item tool-font-adjust-plus flex-center">
            <i class="fas fa-search-plus"></i>
        </li>

        <li class="tools-item tool-font-adjust-minus flex-center">
            <i class="fas fa-search-minus"></i>
        </li>

        <li class="tools-item tool-expand-width flex-center">
            <i class="fas fa-up-right-and-down-left-from-center"></i>
        </li>

        <li class="tools-item tool-dark-light-toggle flex-center">
            <i class="fas fa-moon"></i>
        </li>

        <!-- rss -->
        

        

        <li class="tools-item tool-scroll-to-bottom flex-center">
            <i class="fas fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="exposed-tools-list">
        <li class="tools-item tool-toggle-show flex-center">
            <i class="fas fa-cog fa-spin"></i>
        </li>
        
            <li class="tools-item tool-scroll-to-top flex-center">
                <i class="arrow-up fas fa-arrow-up"></i>
                <span class="percent"></span>
            </li>
        
    </ul>
</div>

    </div>

    
        <aside class="page-aside">
            <div class="post-toc-wrap">
    <div class="post-toc">
        <ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85"><span class="nav-text">环境安装</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#hadoop"><span class="nav-text">hadoop</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Hive"><span class="nav-text">Hive</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#spark"><span class="nav-text">spark</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Spark-%E7%8B%AC%E7%AB%8B%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E7%BC%96%E7%A8%8B"><span class="nav-text">Spark 独立应用程序编程</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%81%87%E5%88%B0%E7%9A%84%E5%9D%91"><span class="nav-text">遇到的坑</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%A4%E7%A7%8D-sparkcontext-%E5%A7%BF%E5%8A%BF"><span class="nav-text">两种 sparkcontext 姿势</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#wordCount"><span class="nav-text">wordCount</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE-vscode-scala-spark-%E7%8E%AF%E5%A2%83"><span class="nav-text">配置 vscode scala spark 环境</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#vscode-%E9%85%8D%E7%BD%AE-scalafmt-%E5%AF%84%E4%BA%86%EF%BC%8C%E5%BB%BA%E8%AE%AE%E7%94%A8-idea"><span class="nav-text">vscode 配置 scalafmt (寄了，建议用 idea)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark-%E8%BF%90%E8%A1%8C%E6%B5%81%E7%A8%8B"><span class="nav-text">Spark 运行流程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RDD"><span class="nav-text">RDD</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#RDD%E5%88%9B%E5%BB%BA"><span class="nav-text">RDD创建</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RDD%E6%93%8D%E4%BD%9C"><span class="nav-text">RDD操作</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#transformation"><span class="nav-text">transformation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#action"><span class="nav-text">action</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RDD%E6%8C%81%E4%B9%85%E5%8C%96"><span class="nav-text">RDD持久化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%94%AE%E5%80%BC%E5%AF%B9-RDD"><span class="nav-text">键值对 RDD</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B1%E4%BA%AB%E5%8F%98%E9%87%8F"><span class="nav-text">共享变量</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F"><span class="nav-text">广播变量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%B4%AF%E5%8A%A0%E5%99%A8"><span class="nav-text">累加器</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark-SQL"><span class="nav-text">Spark SQL</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#DF-%E5%88%9B%E5%BB%BA-amp-%E4%BF%9D%E5%AD%98"><span class="nav-text">DF 创建 &amp; 保存</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%8E%B7%E5%8F%96-mysql-%E6%95%B0%E6%8D%AE"><span class="nav-text">获取 mysql 数据</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark-Streaming"><span class="nav-text">Spark Streaming</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%88%86%E5%8C%BA%E5%99%A8-rdd%E5%A6%82%E4%BD%95%E5%AD%98%E5%82%A8"><span class="nav-text">数据分区器 rdd如何存储</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark-%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB"><span class="nav-text">Spark 源码阅读</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#RDD-1"><span class="nav-text">RDD</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#RDD-%E5%88%86%E5%8C%BA"><span class="nav-text">RDD 分区</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#RDD-%E5%88%86%E5%8C%BA%E4%B8%AA%E6%95%B0%E5%88%86%E9%85%8D%E5%8E%9F%E5%88%99"><span class="nav-text">RDD 分区个数分配原则</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#RDD-%E5%88%86%E5%8C%BA%E5%86%85%E8%AE%B0%E5%BD%95%E4%B8%AA%E6%95%B0"><span class="nav-text">RDD 分区内记录个数</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RDD-%E4%BE%9D%E8%B5%96"><span class="nav-text">RDD 依赖</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#narrow"><span class="nav-text">narrow</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#shuffle"><span class="nav-text">shuffle</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RDD-%E6%8C%81%E4%B9%85%E5%8C%96"><span class="nav-text">RDD 持久化</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#RDD-%E7%BC%93%E5%AD%98%E6%96%B9%E5%BC%8F"><span class="nav-text">RDD 缓存方式</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#persist-cache-unpersist"><span class="nav-text">persist, cache, unpersist</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#iterator"><span class="nav-text">iterator</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#checkpoint"><span class="nav-text">checkpoint</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA"><span class="nav-text">环境搭建</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sparkcontext"><span class="nav-text">sparkcontext</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%89%8D%E8%A8%80"><span class="nav-text">前言</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SparkContext%E7%BB%84%E4%BB%B6%E6%A6%82%E8%A7%88"><span class="nav-text">SparkContext组件概览</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6"><span class="nav-text">任务调度</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Action-%E8%A7%A6%E5%8F%91-Job-submit-%E5%88%B0-DAGScheduler-eventProcessLoop"><span class="nav-text">Action 触发 Job submit 到 DAGScheduler eventProcessLoop</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#DAGScheduler-%E5%AF%B9-Job-%E8%BF%9B%E8%A1%8C-Stage-%E5%88%92%E5%88%86%EF%BC%8C%E5%B9%B6%E4%B8%94%E6%8F%90%E4%BA%A4-Task-%E5%88%B0-TaskScheduler"><span class="nav-text">DAGScheduler 对 Job 进行 Stage 划分，并且提交 Task 到 TaskScheduler</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#TaskScheduler-%E5%B0%86-TaskSet-%E9%80%9A%E8%BF%87-backend-%E7%9A%84%E8%B0%83%E5%BA%A6%E5%88%86%E5%8F%91%E5%88%B0%E4%B8%8D%E5%90%8C%E7%9A%84-executor-%E4%B8%8A%E9%9D%A2"><span class="nav-text">TaskScheduler 将 TaskSet 通过 backend 的调度分发到不同的 executor 上面</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#shuffle-1"><span class="nav-text">shuffle</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%BA%90%E7%A0%81%E9%83%A8%E5%88%86"><span class="nav-text">源码部分</span></a></li></ol></li></ol></li></ol>
    </div>
</div>
        </aside>
    

    <div class="zoom-in-image-mask">
    <img class="zoom-in-image">
</div>


    

</main>




<script src="/js/utils.js"></script>

<script src="/js/main.js"></script>

<script src="/js/header-shrink.js"></script>

<script src="/js/back2top.js"></script>

<script src="/js/dark-light-toggle.js"></script>







    
<script src="/js/code-block-tools.js"></script>





<div class="post-scripts">
    
        
<script src="/js/left-side-toggle.js"></script>

<script src="/js/libs/anime.min.js"></script>

<script src="/js/toc.js"></script>

    
</div>



</body>
</html>
